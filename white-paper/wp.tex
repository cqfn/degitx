\documentclass[12pt,oneside]{article}
\usepackage[utf8]{inputenc}
\usepackage{float}
\usepackage[bottom]{footmisc}
\usepackage{bookmark}
\usepackage{microtype}
\usepackage{amsmath}
\usepackage{multicol}
\usepackage{mdframed}
\usepackage{setspace}
\usepackage{pgfplots}
\usepackage{graphicx}
\usepackage{fancyvrb}
\usepackage{listings}
\usepackage[absolute]{textpos}\TPGrid{16}{16}
\usepackage{tikz}
  \usetikzlibrary{shapes}
  \usetikzlibrary{arrows.meta}
  \usetikzlibrary{arrows}
  \usetikzlibrary{shadows}
  \usetikzlibrary{trees}
  \usetikzlibrary{fit}
  \usetikzlibrary{calc}
  \usetikzlibrary{positioning}
  \usetikzlibrary{decorations.pathmorphing}
\usepackage{./tikz-uml}
\usepackage{everypage}
  \AddEverypageHook{
    \begin{textblock}{0.5}[0,0](0,0)
      \tikz \node[fill=myred,minimum width=0.5\TPHorizModule,minimum height=16\TPVertModule] {};
    \end{textblock}
    \begin{textblock}{0.125}[0,0](0.5,0)
      \tikz \node[fill=myblack,inner sep=0, minimum width=0.125\TPHorizModule,minimum height=16\TPVertModule] {};
    \end{textblock}
  }
\usepackage{xcolor}
  \definecolor{firebrick}{HTML}{B22222}
  \definecolor{myred}{HTML}{CF0A2C}
  \definecolor{myblack}{HTML}{232527}
\newcommand\dd[1]{\colorbox{gray!30}{\texttt{#1}}}
\usepackage{hyperref}
  \hypersetup{colorlinks=true,allcolors=blue!40!black}
\setlength{\topskip}{6pt}
\setlength{\parindent}{0pt} % indent first line
\setlength{\parskip}{6pt} % before par
% \let\oldsection\section\renewcommand\section{\newpage\oldsection}
\date{\small\today}
\title{%
  DRAFT: DeGit - distributed git repository manager\\
  \colorbox{firebrick}{\small\sffamily\color{white}{White Paper}}}
\usepackage[style=authoryear,sorting=nyt,backend=biber,
  hyperref=true,abbreviate=true,
  maxcitenames=1,maxbibnames=1]{biblatex}
  \renewbibmacro{in:}{}
  \addbibresource{books.bib}
\tikzset{node distance=1.6cm, auto, every text node part/.style={align=center, font={\sffamily\small}}}
\tikzstyle{block} = [draw=myblack, fill=white, inner sep=0.3cm, outer sep=0.1cm, thick]
\tikzstyle{ln} = [draw, ->, very thick, arrows={-triangle 90}, every text node part/.append style={font={\sffamily\scriptsize}}]
\tikzstyle{O} = [circle, draw, every text node part/.style={align=center, font={\sffamily\small}}]

% custom commands
\newcommand{\code}[1]{\texttt{#1}}
\newcommand{\todo}[1]{\textcolor{red}{TODO: #1}}

\author{Kirill Chernyavskiy}

\begin{document}
\raggedbottom

\maketitle
\begin{abstract}
  Big software companies or open source communities 
  may have millions of code repositories,
  and use them extensively by programmers and continuous integration (CI) pipelines.
  One git server is not able to satisfy performance expectations,
  many servers with load-balancing can't solve this issue too because
  of inability storage IO scaling for read operations.
  Also, a big company may have distributed teams around the world,
  where each team collaborates with others in one git repo,
  cross-region repo access could be slow in such cases.
  The solution for this problem is distributed git repository storage,
  which replicates repositories across region nodes.
\end{abstract}

\section{Introduction}

\todo{Add introduction}

\section{Technical problems}

There are 3 main probles to solve for distributed git repository manager (DGRM):
\begin{description}
\item[scalability] - one repository may be used actively by many users simultaneously. Storage disk
  has input output (IO) operarion limits, and can't perform a lot of \code{fetch} operations in
  short period of time. The git repository storage should be replicated on different system nodes to
  solve this problem.
\item[dispatching] - this user doesn't know network address of git repository storages. Also, according to the
  previous problem, one repository can be located on multiple replicas.
  So the system should be able to locate and redirect user's requests
  from git client\footnote{Git client is a software that performs git fetch and push operations, it can be
  command line tool, IDE or any graphical user interface communicating with DGRM} to correct git repository
  storage. It should lookup for repository nodes in system and load-balance requests to different storage replicas.
\item[consistency] - the user may \code{push} to repository and \code{fetch} then, in that case it must receive
  same or newer data that user \code{push}ed previously, even if user \code{fetch}ed data from another
  replica of this repository. The linearizability of each single repository in a system is a must have option.
\end{description}

\subsection{Scalability}

Big software companies or big developer's communities have
millions of repositories distributed around the world, and big teams located in different countries
(regions). On high load of \code{fetch} requests git repository storage disks may go above IO operation limits.
Usually, repositories are accessed by different ways:
\begin{description}
  \item[Developer personal activity] - programmers access git repositories via
    \code{git} client and performs \code{fetch}/\code{push} actions. Also, they
    may use web UI interface to upload files, edit files in browser, merge pull requests, etc.
    The number of requests for this activity is not very big according to collected statistics.
  \item[API access] - a lot of services depends on repository management services. It's API robots
    collecting developers activity, background code analyzers of repository, IDEs communicating
    with repositories and reading some history data. The number of such requests is about
    few thousands requests per minute for each million of repositories.
  \item[CI systems] - many events may trigger CI worfkflow, most coommon events are:
    new commit pushed, new pull (merge) request created, new tag pushed, etc.
    On each action CI system downloads the whole repository to run some workflows,
    the download (\code{clone}) operation accompanied by huge bandwidth consumption.
    In additional, some repositories may include submodules, it leads to submodules cloning by CI
    system. According to the statistic, the number is measured as ten thousands requests per minute
    for each million of repositories.
\end{description}

Event if git repository storage is distributed and clients are routed to storage with
correct git repository, the huge amount of \code{fetch} traffic for one repository still able to
make disk go above input output operations (IOP) limits and repository storage will stop
serving requests.

The scalability problem was tried to fix by asynchronous replication of git repository data:
there was proof of concept DGRM with eventually consistency guarantee;
one repository was replicated asynchronously after any update operation to multiple git repository storages,
and clients were load balanced to different git repository storages.
But it was not succeed because of two reasons:
\begin{enumerate}
  \item fetch traffic correlated with push frequency because of huge amount of
  CI systems and API robots involved in development process: each update event usually triggers
  CI build immediately, and CI clones the repository. CI was able to clone only
  primary repository (repository with actual data) because the replication has not been completed
  before CI cloning started.
  \item \code{push} and \code{fetch} frequency was not distributed uniformly over the time - in each
  repository team members may have different responsibilities for review and merge process,
  project technical lead can merge all approved pull-requests in short period of time
  which causes frequent push operations in git repo.
\end{enumerate}

These two conditions lead to high fetch traffic peaks for git repositories:
frequent push operations turn replication storages into inconsistent state,
and lead to high fetch traffic to primary repository storages from CI systems which clones these repositories.
It causes the same problems as were common for non-replicated system - the primary node goes beyond IOPS limits and
rejects new \code{fetcj} requests.

Futhermore, \code{merge} is not the only way to ``update'' the repository. Lots of other activities can do that,
here is the list of some:

\begin{description}
  \item[branches changing] \verb|create|/\verb|delete|/\verb|update| branches by git push or by using web page.
  \item[tags changing] \verb|create|/\verb|delete|/\verb|update| tags by git push or by releasing new version on web page.
  \item[special refs changing] when new merge request is created, a new\\
    \emph{\code{refs/merge-requests/IID\footnote{Internal id of a project on gitlab}/head}} named ref
    is created. When source branch of the merge request is updated, the ref is also updated.
  \item[migrations] sometimes repository administrator can migrate the repository to
    another physical device or another region node, it also could be treated as an update.
    Also, delete a repository could be treated as migrate it to a trash area.
\end{description}

\subsection{Dispatching}

Git client doesn't know where repository replica is located in the system. Dispatching algorithm
should be able to locate correct storage nodes (distributed system nodes) of repository and redirect
each client's request to one of these nodes. Moreover, it's not enough just to redirect client's request,
the dispatching endpoint should load-balance all requests for each particular repository to distribute
storage load over the time. Round-robin load-balancing solves this problem, since the scalability problems
occurs only on peeks of high read traffic, so statistically redirecting each next read request to
different storage node reduces disk load by \code{n}-times, where \code{n} is amount of storage replicas.

\subsection{Consistency}

\todo{Describe the problem}

% \subsection{Government restrictions}
% The most popular repository management systems are \href{https://github.com/}{GitHub} and
% \href{https://about.gitlab.com}{GitLab} are under control of government, these servcice may
% restric access for some groups of people based on their location and may reject to provide paid
% versions for hosting internal servers. \todo{add more details}.

\section{Solution}

The solution is a distributed Git repository manager with strong consistent replica nodes.
It consists of two parts: the back-end and front-end (see Figure ~\ref{fig:comp-arc-overview} diagram):

\begin{description}
  \item[The back end] - (core network, storage), P2P\footnote{peer to peer} system which stores git data
    and metadata\footnote{Here git data referenced to git objects, and git metadata to git references.
    In this document git data usually refer to both objects and references unless otherwise stated},
    it's responsible for replication of repositories, and guarantee
    strong consistency of git storage across replicas. It exposes internal RPC API to accept git intermediate
    requests, which can modify repository state. And provide endpoints to read (\code{fetch}) repository data.
  \item[The front end] - (multiplexer, load balancer). It exposes public API for all git operations,
    translates all operations request into intermediate RPC language, routes requests to proper storage
    nodes and load-balancing read (\code{fetch}) requests to different replicas of same repository.
\end{description}

\begin{figure}
  \begin{center}
    \begin{tikzpicture}
      \umlbasiccomponent[x=-3.5]{Client}

      \begin{umlcomponent}[x=1, y=-4]{Front-end}
        \umlbasiccomponent{Load-balancer}
        \umlprovidedinterface[interface=Entry point, name=lb-e6ce, padding=2cm]{Load-balancer}
        \umlbasiccomponent[y=-3]{Routing table}
      \end{umlcomponent}
      \umlport{Front-end}{west}
      \umldelegateconnector{Front-end-west-port}{lb-e6ce}
      \umlHVHassemblyconnector[interface=RPC, with port, name=fe-rpc,
        middle arm, arm1=-1cm, anchor1=-180]{Client}{Front-end}

      \begin{umlcomponent}[x=6, y=-1]{Back-end}
        \umlbasiccomponent{git storage}
      \end{umlcomponent}
      \umlHVassemblyconnector[interface=RPC, name=be-rpc, with port]{Front-end}{Back-end}

      \umlnote[x=1.2]{fe-rpc-interface}{Front-end exposes public API for git operations}
      \umlnote[x=7, y=-7]{be-rpc-interface}{Back-end exposes internal API for storage operations}
    \end{tikzpicture}
  \end{center}
  \caption{
    Components architecture overview:
    Each client C connected to DeGit front-end component,
    the front-end has load-balancer to proxy incoming requests to back-end,
    it finds back-end nodes using routing table.
  }
  \label{fig:comp-arc-overview}
\end{figure}

The front end exposes public API for well-known protocol, e.g. one front-end implementation provides
\href{https://docs.gitlab.com/ee/administration/gitaly/}{Gitaly} gRPC interfaces for GitLab
instance set\footnote{In this document GitLab instance set is a set of shell, workhorce and web components of GitLab},
see Figure ~\ref{fig:gitlab-set} diagram. There GitLab is a client of DeGitX front-end, it sends gRPC requests to
front-end to modify and fetch git data.

\begin{figure}
  \begin{center}
    \begin{tikzpicture}
      \umlbasiccomponent[name=git, y=-0.5]{git CLI}
      \umlbasiccomponent[name=browser, y=-5]{Browser}

      \begin{umlcomponent}[name=gitlab, x=8]{GitLab}
        \umlbasiccomponent[name=gl-shell]{shell}
        \umlprovidedinterface[interface=SSH, name=gl-shell-ssh, padding=2cm, distance=2.5cm]{gl-shell}

        \umlbasiccomponent[name=gl-workhorse, y=-3]{Workhorse}
        \umlprovidedinterface[interface=HTTP, name=gl-wh-http, padding=2cm, distance=2.5cm]{gl-workhorse}

        \umlbasiccomponent[name=gl-web, y=-6]{Web}
        \umlprovidedinterface[interface=HTTP, name=gl-web-http, padding=2cm, distance=2.5cm]{gl-web}
      \end{umlcomponent}
      \umlport{gitlab}{150}
      \umldelegateconnector{gitlab-150-port}{gl-shell-ssh}
      \umlport{gitlab}{210}
      \umldelegateconnector{gitlab-210-port}{gl-wh-http}
      \umldelegateconnector{gitlab-210-port}{gl-web-http}
      \umlassemblyconnector{git}{gitlab-150-port}
      \umlassemblyconnector{git}{gitlab-210-port}
      \umlassemblyconnector{browser}{gitlab-210-port}
      \umlport{gitlab}{east}

      \umlbasiccomponent[name=gitaly-fe, x=8, y=-10]{DeGitX front-end}
      \umlport{gitaly-fe}{east}
      \umlHVHassemblyconnector[interface=Gitaly gRPC, arm1=1cm]{gitlab-east-port}{gitaly-fe-east-port}
    \end{tikzpicture}
  \end{center}
  \caption{
    GitLab set and Gitaly front-end:
    git client communicates with GitLab-shell via SSH or with GitLab-workhorse via HTTP(S).
    Browser uses GitLab-web. All three GitLab components communicates with git storage via DeGitX load balancer
    (front-end) via gRPC protocol defined by Gitaly component of GitLab.
  }
  \label{fig:gitlab-set}
\end{figure}

The communication layer of DeGit front-end and back-end is DeGit metadata: it consists of mapping
of repository hash to storage node locator, the front-end can be configured differently depends on
setup kind, it can query metadata from database, it can query lookup it via DHT\footnote{Distributed hash table}
or it can receive metadata updates broadcast from storage node peers in local network via UDP protocol,
see \ref{sec:metadata}.

When a client (e.g. GitLab-shell) write git data to the system (via \code{push}), the request is routing
by front-end load balancer to one of the storage replicas.
Storage back-end node starts leader election with other repository replicas and updates the log\footnote{Here: node log
or back-end log is distributed replicated log of state machine associated with certain git repository storage}
of consensus of repository holders (replicas), see \ref{sec:data}.

The system autmatically rebalances repository storage: when repository is not used actively for a
long time, the node can remove a repository from storage if 3 replicas of this repository exists on other nodes;
if some node has a lot of free space on storage, and storage of another node is almost full,
full node can transfer (move) some repositories to another node, and IOPS of storage device is also
a importance measure.

The system can accept new nodes and automatically fill it with replica repository and
move some repository to new node. Same for disconnecting: if node was disconnected from the system or crashed,
it creates additional replicas on nodes to have at least 3 replicas for each repository in a system.

\subsection{Protocol}
Main system protocols are:
\begin{description}
  \item[Location protocol] - unique node identity.
  \item[Network protocols] - nodes communication protocols.
  \item[Discovery protocol] - lookup of node real address by locator ID.
  \item[Metadata exchange protocol] - a mapping of git repository to storage back-end node.
  \item[Data exchange protocol] - Git objects and references exchange protocol,
    commands to add new objects and update references, linearizability guarantee.
\end{description}

\subsubsection{Locators}
\label{sec:locators}
Network addresses are not stable, back-end node may get network address via
Dynamic Host Configuration Protocol (DHTP), or node owner may move it from one server to another.
So we can't rely on network addresses when working with back-end nodes, we need some
overlay networks and unique identifiers for each node. To identify a node DeGit uses
public-key cryptography: node owner generates private and public key pair using one of the supported
crypto algorithms \todo{which exactly?}, these keys will identify back-end node uniquelly.
Node uses cryptographic hash \todo{choose algorithm} of public key as node locator ID,
as reseearched in the ~\cite{securebaserouting}. Nodes uses locator IDs to introduce
itself to the system and build overlay network on top of real network. DeGit uses
Multihash\footnote{https://multiformats.io/multihash/} format to encode locator IDs.

Private and public keys can be used to sign requests to other nodes or to build
trusted discovery (see ~\ref{sec:discovery}) point in a system: when a new system is created,
administrator can create certification authority (CA) for issuing digital certificates for node
public keys, so each node will be able to verify certificate of any other node when
using seed list URLs or other peers exchange algorithms. Node instances uses locators to talk
with each other, it can find real network address of nodes using this locator ID.
Also, system uses locators for node-repository mapping (see ~\ref{sec:data}),
the item of mapping table consists of locator ID and unique repository name hash.

\subsubsection{Network}
\label{sec:network}

DeGitX uses TCP or UDP protocols for transport layer, TCP is used by default. It uses IPv4 or IPv6 for addressing,
it stores addresses in \href{https://github.com/multiformats/multiaddr}{multiaddr} format, for instance:
\code{/ipv4/1.2.3.4/udp/4444} evaluates to \code{4444} UDP port on \code{1.2.3.4} IPv4 address.
DeGitX uses a routing system (see \ref{sec:discovery}) to find addresses of nodes by locators.

\subsubsection{Discovery}
\label{sec:discovery}

Each node doesn't know network addresses of others by default, it knows only locator ID.
Different discovery techicues are used for node lookup, it can be configured independenly by node
administrator and they could be used together:

\begin{description}
  \item[LPD] - local peer discovery: each peer sends UDP messages with locator ID.
    It's cheap and fast for network layer (due to UDP messages); other system components,
    such as front-ends and back-ends receive these messages and update local cache of node locators.
    These approach dramatically improve lookup performance in local networks, e.g. when most of communications
    are performed in one local region, and all region nodes are connected via local networ.
    This protocol supposed to be used with other discovery dechicues.
    For security reasons, in untrusted networks, the broadcast messages may be optionally signed with
    node private keys to be verified by the receiver using CA public certificate.
    For optimization reasons, this protocol is resused by metadata exchange protocol for
    repository hash table propagation (see \ref{sec:metadata}). \todo{lookup for researches}.
  \item[Distributed DB] - network addresses could be propagated to the system using supplimentary distributed
    database, e.g. etcd or others. On startup, the back-end node registers itself in database,
    and other system nodes (both front-ends and back-ends) uses this database for lookups. 
    It requires additional system configuration, but delegating some responsibilities to third-party services.
  \item[Seed hosts] - nodes can use other nodes as seed hosts as caching optimization, some nodes
    will be responsible for caching lookup results. In other words, these seed nodes represents similar abstraction
    as Content Delivery Network (CDN) in web caching.
  \item[DHT] - distributed hash tables uses different metrics to compare the distance of nodes in overlay network.
    For example Kademlia\footnote{pdos.csail.mit.edu/~petar/papers/maymounkov-kademlia-lncs.pdf} uses
    \code{XOR} metric of node ID for distance measurement. It requires node IDs to be widely distributed -
    DeGit locators satisfies this requirements, being generated as cryptographic hash function from
    public key. In Kademlia lookup system each node stores references in K-buckets, where each K-bucket
    contains node addresses with same ID prefix.
\end{description}

Depends on configuration node may use or may not some discovery protocols. They are performed
in well specified order: firstly, the node is lookup for locator ID in local cache, the cache is updated
by LPD broadcasts; in case if not found, the node is quering discovery database, then seed hosts, and DHT
as last resort.

The structure of discovery entries is a mapping of locator ID (which is cryptographic hash by design)
to network address in Multiaddr format (as described in network section: ~\ref{sec:network}).

Example of routing table:

\begin{tabular}{l | l}
  Locator ID & Node address \\ \hline
  \code{122041dd7b6443...0022ab11d2589a8} & \code{/ipv4/192.168.1.42/tcp/9031} \\
  \code{122041dd7b6443...0022ab11d2589a8} & \code{/ipv4/192.168.1.33/tcp/8011} \\
  \code{132052eb4dd19f...6f8c7d235eef5f4} & \code{/ipv4/172.18.11.22/tcp/9031} \\
\end{tabular}

\subsection{Metadata}
\label{sec:metadata}

DeGit peers doesn't know where to find repository storage by default,
to solve it, the system introduces metadata layer to exchange repository coordinates
to locator IDs. The structure of metadata is a many-to-many relation of
repository cryptographic hash to storage locator ID.

For instance, here is the example of metadata of two repositories located at two nodes,
repository \code{repo1} is located on both nodes \code{node1} and \code{node2},
the repository \code{repo2} is located only on node \code{node2}:

\begin{tabular}{l | l}
  Repository hash & Locator ID \\ \hline
  \code{hash(repo1)} & \code{locator(node1)} \\
  \code{hash(repo1)} & \code{locator(node2)} \\
  \code{hash(epo2)} & \code{locator(node2)} \\
\end{tabular}

The repository hash is encoded in Multihash format. Metadata exchange protocol partially reuses
discovery protocol for network optimizations, e.g. peers sends local broadcasts
with locator IDs and known repository hashes, discovery database may keep (if configured)
repository hash map to node locator IDs relations (see \ref{fig:repo-lookup-db}),
DHT keeps locator IDs as a value for repository hash keys \ref{fig:repo-lookup-dht},
it keeps all metadata of the whole system. The metadata lookup process is similar to discovery protocol:
firstly, peer is looking for metadata in local cache (populated with network broadcasts); then, it checks
region database; and as last resort, it performs query lookup for global DHT.

When a new node starts replicating some repository, it's synchronizing with other replicas first, and then
updates metadata asynchronously. It's how metadata is updated:
\begin{enumerate}
  \item A new node wants to replicate some repository
  \item The node finds current repository holders (replicas) in existing metadata and choose random node from this list
  \item A node sends the request to the node to add itself to the replica list
  \item Receiver node starts leader election and updates node log to add new node to the replica list
  \item The consensus accepts new node and stores it in local persistant storage
  \item The leader notifies a new node that it becomes a part of the replicas and consider it when consensus is required
  \item New node replicates the state of consensus and holds the repository and sends UDP broadcast to
    local peers on success
  \item Leader node propagates replicas change in metadata storages asynchronously: updates DHT or database storage
    (as configured)
\end{enumerate}

Repository mapping could be stored in distributed hash table (e.g. Kademlia),
database cache or broadcasted via local network broadcasts.
E.g. for multi-region cluster setup, the front-end load balancer may look for node in local cache
populated by local network broadcasts, then check region database where all repositories in same region are
registered, and if not found perform DHT query lookup in different regions,
see Figures \ref{fig:repo-lookup-db} \ref{fig:repo-lookup-dht}.

\begin{figure}
  \begin{center}
    \begin{tikzpicture}
      \node[block] (db) {DB};
      \node[block, below=of db, xshift=-3cm] (fe) {Front-end};
      \node[O, below=of db, xshift=3cm] (Nx) {N$_{x}$};

      \draw[<->] (fe) -- node {1} (db);
      \draw[<->] (fe) -- node {2} (Nx);
    \end{tikzpicture}
  \end{center}
  \caption{
    Repository lookup in database:
    DB - Database with metadata for current region.
    1 step - front-end load balancer query database for repository metadata.
    2 step - front-end access node N$_{x}$ with required repository.
  }
  \label{fig:repo-lookup-db}
\end{figure}

\begin{figure}
  \begin{center}
    \begin{tikzpicture}
      \matrix[row sep=10mm](feg){
        \node[block] (fe) {Front-end}; \\
      };
      \matrix[row sep=3mm, column sep=5mm, right=of feg](r1){
        \node[O] (r1n1) {N$_{r1,1}$}; \\
        \node[O] (r1n2) {N$_{r1,2}$}; \\
        \node[O] (r1n3) {N$_{r2,3}$}; \\
        \node[O] (r1nN) {N$_{r1,n}$}; \\
      };
      \node[fit=(r1)(feg), draw, dashed, label={[right=0cm, above=3cm]1cm:R$_{1}$}](r1g){};
      \matrix[row sep=3mm, column sep=5mm, right=of r1](r2){
        \node[O] (r2n1) {N$_{r2,1}$}; \\
        \node[O] (r2n2) {N$_{r2,2}$}; \\
        \node[O] (r2nN) {N$_{r2,n}$}; \\
      };
      \node[fit=(r2), draw, dashed, label={[right=0cm, above=3cm]1cm:R$_{2}$}](r2g){};
      \matrix[row sep=3mm, column sep=5mm, right=of r2](r3){
        \node[O] (r3n1) {N$_{r3,1}$}; \\
        \node[O] (r3n2) {N$_{r3,2}$}; \\
        \node[O] (r3n3) {N$_{r3,3}$}; \\
        \node[O] (r3nN) {N$_{r3,n}$}; \\
      };
      \node[fit=(r3), draw, dashed, label={[right=0cm, above=3cm]1cm:R$_{3}$}](r3g){};
      \draw[-latex] (fe) -- (r1n2);
      \draw[-latex] (r1n2) -- (r2n1);
      \draw[-latex] (r2n1) -- (r3n3);
    \end{tikzpicture}
  \end{center}
  \caption{
    Repository lookup via DHT:
    Front-end in Region R$_{1}$ unable to find repository located on node N$_{r3,3}$ in same region,
    but it query the nearest known node N$_{r2,1}$, the node N$_{r2,1}$ knows where repository is located
    and redirects the query to node N$_{r3,3}$ with repository.
  }
  \label{fig:repo-lookup-dht}
\end{figure}

\subsection{Security}
\label{subsec:security}
A small fraction of malicious nodes can prevent correct message delivery throughout the overlay.

Such nodes may mis-route, corrupt, or drop messages and routing information.
Additionally, they may attempt to assume the identity of other nodes and corrupt or delete objects
they are supposed to store on behalf of the system.

All attacks based on presence of malicious nodes.
There are 2 ways:
\begin{itemize}
  \item[$-$] Implement techniques that allow nodes to join the overlay, to maintain routing state, and to forward messages securely in the presence of malicious nodes.
  \item[$-$] Make the appearance of malicious nodes impossible.
\end{itemize}

Degitx is not an open peer-to-peer system
where resource pooling without preexisting trusted relationships is possible.
It means that untrusted nodes aren't allowed to join
and all members of the network are trusted not to cheat.

To be sure that node is trusted, it's nodeId certificates should be signed by trusted CAs.
Then each node rejects all unsigned requests.

Certified nodeIds work well when nodes have fixed nodeIds.
This condition is met while Node uses cryptographic hash of public key as nodeId.

These certificates give the overlay a public key infrastructure,
suitable for establishing encrypted and authenticated channels between nodes.
Nodes with valid nodeId certificates can join the overlay, route messages,
and leave repeatedly without involvement of the CAs.

When the membership of a peer-to-peer system is constraint and all nodes are trusted as in DeGitX,
CAs could solve security issues, because, according to the ~\cite{securerouting} research, all attacks are based on malicious nodes presence.

\subsection{Git data exchange}
\label{sec:data}
Consensus could be implemented using \emph{\href{https://raft.github.io/raft.pdf}{Raft}} or
\emph{\href{http://www.cs.yale.edu/homes/aspnes/pinewiki/Paxos.html}{Paxos}} algorithm.

\todo{search for more algorithms}.

\section{Explanation}

\subsection{How the client find replica with git repository}

\subsection{How the client pushs new commit to git repository}

\subsection{How the client fetches git data from repository}

\subsection{How new replica node connected to the cluster}

\todo{explain all questions}

\subsection{Git Internals}
\subsubsection{Dictionary}
\label{subsec:git-internals}
\begin{description}
  \item[Git] is a simple key-value data store and stores 3 main types of Objects: blob, tree, commit, and one additional: tag.
  \item[Blob] - content of file w/o filename.
  \item[Tree] - A single tree object contains one or more entries, each of which is the SHA-1 hash of a blob or subtree with its associated mode, type, and filename.
  For example, the most recent tree in a project may look something like this:\\
  \\
  \code{\$ git cat-file -p master\^\{tree\}}\\
  \code{100644 blob a906cb2a4a904a152e80877d4088654daad0c859      README}\\
  \code{100644 blob 8f94139338f9404f26296befa88755fc2598c289      Rakefile}\\
  \code{040000 tree 99f1a6d12cb4b6f19c8655fca46c3ecf317074e0      lib}
  \begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{data-model-1.png}
    \label{fig:tree-objects}
  \end{figure}
  \item [Commit] is just a pointer to parent(previous) commit if any
  and to top-level tree for current one with some metadata
  (the author information and so on).
  \begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{data-model-3.png}
    \label{fig:commit-objects}
  \end{figure}
\end{description}

{\large Other things, listed below, serve to make our interaction with these objects easier.
}
\begin{description}
  \item[Reference] - A file in which you could store SHA-1 value
  of a commit under a simple name, so you could use that simple name
  rather than the raw SHA-1 value.
  \item[Branch] is a simple pointer or reference to the head of a line of work(the latest commit) in .git/refs/heads.
  Branch name is mapped to the SHA of the commit that is latest for this branch.
  \begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{data-model-4.png}
    \label{fig:reference}
  \end{figure}
  When you run commands like \code{git branch <branch>},
  Git basically runs update-ref command to add the SHA-1
  of the last commit of the branch you're on into whatever
  new reference you want to create.
  So create branch is just create a reference to some commit.
  And delete branch is only delete reference.
  After branch deletion, all its objects will be available via their SHA.
  The same is for other reference under .git/refs
  \item[HEAD] Usually the HEAD file is a symbolic reference to the branch
  you're currently on.
  By symbolic reference, we mean that unlike a normal reference,
  it contains a pointer to another reference.\\
  If you look at the file, you'll normally see something like this:\\
  \\
  \code{\$ cat .git/HEAD}\\
  \code{ref: refs/heads/master}\\
  \\If you run \code{git checkout branch}, Git updates the file to look like this:\\
  \\
  \code{\$ cat .git/HEAD}\\
  \code{ref: refs/heads/test}\\

  When you run \code{git commit}, it creates the commit object,
  specifying the parent of that commit object to be whatever SHA-1 value
  the reference in HEAD points to.
  This commit also becomes a new head for a current branch.
  refs/heads/<branch-name> will be updated and map to the SHA
  of this new commit.
  When you do \code{git reset HEAD~1},
  it also only update refs/heads/<branch-name>.
  It takes SHA of parent commit of current head commit.
  \item[Tag (lightweight)] It's like a branch reference, but it never moves - it always points to the same commit but gives it a friendlier name.\\
  You can create, update and delete such a tag, and no objects would be affected.
  It's only a reference.
  \item[Tag (annotated)] - tag object, that points to commit or any other object amd contains metadata, and a reference to this tag object.

  Use \code{git push --follow-tags} if you want to push your annotated tag to remote repo.
  To delete a tag on your local repository, you can use \code{git tag -d <tagname>}.
  To delete a remote tag: \code{git push origin --delete <tagname>}.

  Both operation only delete a reference in .git/refs/tag.
  Tag object wouldn't be removed.
\end{description}

\subsubsection{Objects update workflow}
The most important updates of remote repo for us listed in the table below:

\begin{table}[H]
  \hskip-2.0cm\begin{tabular}{|l|l|l|}
                \hline
                action                                                               & git commands                                                                                                                                                                                                                                    & git server changes                                                                                                                                                                                                                                        \\ \hline
                create branch                                                        & \begin{tabular}[c]{@{}l@{}}git push  \textless{}remote-name\textgreater \\  \textless{}branch-name\textgreater{}:\textless{}remote-branch-name\textgreater{}\end{tabular}                                                                       & \begin{tabular}[c]{@{}l@{}}A new reference \\ .git/refs/heads/\textless{}branch-name\textgreater \\ will be created.\end{tabular}                                                                                                                         \\ \hline
                delete branch                                                        & \begin{tabular}[c]{@{}l@{}}git push  \textless{}remote-name\textgreater \\ :\textless{}remote-branch-name\textgreater\\ OR\\ git push \textless{}remote-name\textgreater  \\ --delete  \textless{}remote-branch-name\textgreater{}\end{tabular} & \begin{tabular}[c]{@{}l@{}}The existing reference \\ .git/refs/heads/\textless{}branch-name\textgreater \\ will be removed.\end{tabular}                                                                                                                  \\ \hline
                create tag                                                           & \begin{tabular}[c]{@{}l@{}}git push \textless{}remote-name\textgreater \\ \textless{}tag-name\textgreater{}:\textless{}remote-tag-name\textgreater{}\end{tabular}                                                                               & \begin{tabular}[c]{@{}l@{}}A new reference \\ .git/refs/tags/\textless{}tag-name\textgreater \\ will be created.\\ For annotated tags only:\\ new tag-object also will be created\\ .git/objects/SHA{[}0,2{]}/SHA{[}2,40{]}\end{tabular}                  \\ \hline
                delete tag                                                           & git push \textless{}remote-name\textgreater :\textless{}tag-name\textgreater{}                                                                                                                                                                  & \begin{tabular}[c]{@{}l@{}}The existing reference \\ .git/refs/tags/\textless{}tag-name\textgreater\\  will be removed.\\ For annotated tags:\\ tag-object won't be removed or updated\\ For lightweight tags:\\ tag-object doesn't exist\end{tabular}    \\ \hline
                \begin{tabular}[c]{@{}l@{}}create ref\\ delete ref\end{tabular}      &                                                                                                                                                                                                                                                 & \begin{tabular}[c]{@{}l@{}}Reference is fully equal to branch,\\  it's just an alias.\end{tabular}                                                                                                                                                        \\ \hline
                \begin{tabular}[c]{@{}l@{}}update branch\\ (new commit)\end{tabular} & git push                                                                                                                                                                                                                                        & \begin{tabular}[c]{@{}l@{}}New objects will be created \\ (commit, tree, etc.).\\ The existing reference \\ .git/refs/heads/\textless{}remote-branch-name\textgreater \\ will be updated.\\ It will start pointing \\ to to this new commit.\end{tabular} \\ \hline
  \end{tabular}\label{tab:update-workflow}
\end{table}

\subsubsection{Git file storage internals}

\textbullet\textbf{\large{ loose object storage}}\\

As we metioned, a git repository take cares of several types of objects:
\code{blob}, \code{tree}, \code{commit}, \code{tag}. Let's take a look at the details of object storage:\\

git default write the objects to directory \code{GIT\_DIR\footnote{the .git directory}/objects}.
Each object is named by a sha1 value for example \emph{\code{b98191cf971f2418e42877410a6c40fc112a0a93}}\\
it's storage path will be
\begin{center}
\emph{\code{GIT\_DIR/objects/b9/8191cf971f2418e42877410a6c40fc112a0a93}}
\end{center}
the directories under \code{GIT\_DIR/objects} is named by \code{SHA[0,2]} of objects' sha1 value,
this can avoid too much files in one directory because some filesystem has the max link number of files,
such as ext3 has the definition:
\begin{flushleft}
  \code{include/linux/ext3\_fs.h:\#define EXT3\_LINK\_MAX           32000}
\end{flushleft}

One object file is consist of three portions:
\begin{description}
  \item[type] can be one of `blob', `tree', `commit', `tag'
  \item[size] the number of bytes of the content
  \item[content] the content of object
\end{description}

and the object file will be compressed using zlib:

\begin{tikzpicture}
  \node at (0,0.25) {binary};
  \node at (4,0.25) {\code{7370 6163 657d 0a5c ...}};
  \draw (1,0) rectangle (7,0.5);
  \node at (3.1,1.25) {compress};
  \draw[-latex] (2,1.5) -- (2,1);
  \node at (0,2.25) {buffer};
  \node at (1.5,2.25) {type};
  \node at (3.5,2.25) {size};
  \node at (6,2.25) {content};
  \draw (1,2) rectangle (2.95,2.5);
  \draw (3,2) rectangle (4.95,2.5);
  \draw (5,2) rectangle (10,2.5);
\end{tikzpicture}

the compression level can be set by `\textbf{\code{core.compression}}' or `\textbf{\code{core.loosecompression}}'.
\\

\textbullet\textbf{\large{ pack file storage}}\\

When we do \emph{\code{git gc}} or \emph{\code{git repack}}, pack files with the suffix \code{.pack}
maybe be created under directory \emph{\code{GIT\_DIR/objects/pack}}. A pack is a collection of objects,
individually compressed, with delta compression applied, stored in a single file, with an associated
index file. Packs are used to reduce the load on mirror systems, backup engines, disk storage, etc.

\begin{lstlisting}[basicstyle=\ttfamily]
$ tree .git/objects/pack
.git/objects/pack
|-- pack-24fcb83682e5a2848ef7bd00a9eda0bff1a372fc.idx
|-- pack-24fcb83682e5a2848ef7bd00a9eda0bff1a372fc.pack
|-- pack-915d8b4ae03b03179912c589cee932e5a990b7f0.idx
|-- pack-915d8b4ae03b03179912c589cee932e5a990b7f0.pack
|-- ...
\end{lstlisting}

Conceptually there are only four object types in a pack file: commit, tree, tag and blob.
However to save space, an object could be stored as a ``delta'' of another ``base'' object.
These representations are assigned new types \code{ofs-delta} and \code{ref-delta}.

\begin{description}
  \item[delta object]
    Both ofs-delta and ref-delta store the ``delta'' to be applied to another object
    (called `base object') to reconstruct the object. The difference between them is,
    ref-delta directly encodes 20-byte base object name. If the base object is in the same pack,
    ofs-delta encodes the offset of the base object in the pack instead.
  \item[base object]
    The base object could also be deltified if it's in the same pack.
    Ref-delta can also refer to an object outside the pack. When stored on disk however,
    the pack should be self contained to avoid cyclic dependency.
\end{description}

And usualy we have lots of versions of a single file in a repository, a new pack file will
store the last version's content as the base object, and computes the earlier versioins to
to be delta objects and store them referring to the base object:

\begin{tikzpicture}
  % loose objects
  \filldraw[fill=yellow, draw=white] (3.75,2.5) rectangle (8.5,5.5);

  \node at(0,5){v3};
  \node[rectangle,rounded corners, draw=gray,] (v3) at(2,5){README.md};
  \node[rectangle,rounded corners, draw=gray,] (v3obj) at(6,5){.git/objects/77/652063...};
  \node at(0,4){v2};
  \node[rectangle,rounded corners, draw=gray,] (v2) at(2,4){README.md};
  \node[rectangle,rounded corners, draw=gray,] (v2obj) at(6,4){.git/objects/6e/6f7420...};
  \node at(0,3){v1};
  \node[rectangle,rounded corners, draw=gray,] (v1) at(2,3){README.md};
  \node[rectangle,rounded corners, draw=gray,] (v1obj) at(6,3){.git/objects/28/753a20...};
  \draw (v3) -- (v3obj);
  \draw (v2) -- (v2obj);
  \draw (v1) -- (v1obj);

  % arrow
  \draw (8.5,2) -- (8.5,0.35);
  \draw[-latex] (8.5,0.35) -- (6.5,0.35);
  \node at(7,1.5){pack-objects};

  % pack file
  \filldraw[fill=yellow, draw=white] (0.8,0) rectangle (6.2,0.75);
  \node at (0,0.35) {pack file};
  \node (base) at (2.5,0.35) {base object};
  \node (deltav2) at (4.5,0.35) {delta};
  \node (deltav1) at (5.5,0.35) {delta};
  \draw (1,0.1) rectangle (3.95,0.6);
  \draw (4,0.1) rectangle (4.95,0.6);
  \draw (5,0.1) rectangle (5.95,0.6);

  \draw[dashed, ultra thick, -latex] (v3obj) -- (base);
  \draw[dashed, very thick, -latex] (v2obj) -- (deltav2);
  \draw[dashed, -latex] (v1obj) -- (deltav1);
\end{tikzpicture}

Usually objects contained in the pack are compressed using zlib, the compression level
can be set by `\textbf{\code{core.compression}}' or `\textbf{\code{pack.compression}}'.

When objects contained in the pack are stored using delta compression. The objects
are first internally sorted by type, size and optionally names and compared against
the other objects to see if using delta compression saves space. `\textbf{\code{pack.depth}}'
limits the maximum delta depth; Making it too deep affects the performance on the unpacker side,
because delta data needs to be applied that many times to get to the necessary object.
\\

To quickly find objects in the pack file, an associated index file is created with the
\emph{\href{https://github.com/git/git/blob/master/Documentation/technical/pack-format.txt\#L161}{format}},
And `\emph{\code{git index-pack}}' can be used to regenerate the *.idx file on the *.pack file.
\\

\textbullet\textbf{\large{ when pack file is created}}\\

When we talk about the pack files, we said that packs are used to reduce the load on mirror systems,
backup engines, disk storage, etc. So pack files will created in several scenes:

\begin{description}
  \item[git gc] `\emph{\code{git gc}}' runs a number of housekeeping tasks within the repository,
    such as compressing file revisions (to reduce disk space and increase performance), removing
    unreachable objects which may have been created from prior git commands.
    We can run it manually, and it may create a new pack file.
  \item[git repack] `\emph{\code{git repack}}' is used to combine all objects that do not currently
    reside in a pack file into a pack. It can also be used to re-organize existing packs into
    a single, more efficient pack.
  \item[git push] `\emph{\code{git push}}' will run \code{send-pack} to connects to the remote side,
    it gets one file descriptor which is either a socket (over the network) or a pipe (local).
    What's written to this file descriptor goes to `\emph{\code{git-receive-pack}}' to be unpacked.
    We can get the following pipeline flow from\\
    \emph{\href{https://github.com/git/git/blob/master/Documentation/technical/send-pack-pipeline.txt}{Documentation/technical/send-pack-pipeline.txt}}:
    \begin{lstlisting}[basicstyle=\ttfamily]
    send-pack
       |
       pack_objects() --> fd --> receive-pack
          | ^ (pipe)
          v |
       (child)
    \end{lstlisting}
    so we can see that `\emph{\code{git push}}' will create a pack file and send it to the remote side.
  \item[git-receive-pack] As menthioned above, `\emph{\code{git-receive-pack}}' will receive a pack file
    from `\emph{\code{git push}}'. And `\emph{\code{git receive-pack}}' will also forked a child
    `\emph{\code{git gc --auto --quiet}}' to check if there are too many loose objects to pack.
    Ususaly the threshold is 6700 loose files, we can set it by `\textbf{\code{gc.auto}}'.
  \item[git-upload-pack] When clients runs `\emph{\code{git clone}}' or `\emph{\code{git fetch}}', the
    client connects to the remote side and invokes `\emph{\code{git-upload-pack}}'.
    `\emph{\code{git-upload-pack}}' will communicate with client and compute how many objects the client
    want then fork and exec child `\emph{\code{git-pack-objects}}' to create a pack file for all the
    needed objects and pipe the pack file to client.
    \begin{lstlisting}[basicstyle=\ttfamily]
    upload-pack
       |
       pack_objects() --> fd --> fetch-pack
          | ^ (pipe)
          v |
       (child)
    \end{lstlisting}

    We can also use the `\textbf{\code{repack.writeBitmaps}}' to let git write a bitmap index when packing
    all objects to disk (e.g., when git repack -a is run). This index can speed up the "counting
    objects" phase of subsequent packs created for clones and fetches, at the cost of some disk space
    and extra time spent on the initial repack.

\end{description}

\subsubsection{what makes repository slow}

\todo{}

\section{Requirements}
\label{sec:requirements}

\subsection{Features}
\label{sec:features}

The most critical
\href{https://en.wikipedia.org/wiki/Non-functional_requirement}{Non-functional requirements}
are:

\begin{description}
  \item[Read scalability]
    The solution should scale out the read capacity of a system, each region should be able
    to access repository using most available replica node.
  \item[Strong consistency]
    All? (\todo{discuss, maybe not all but the majority of replicas})
    active replica repositories should be synchronized on updates in any node
    with immediate consistency.
  \item[Durability]
    The system must have enough replicas to recover itself in case of corruption.
    Corrupted repository could be responsible for recovering itself using replica nodes.
  \item[Self management (rename?)]
    Each node performs cleanup when needed (\code{git gc}) and may remove replica
    from storage on read inactivity.
    A node should be able to find and synchronize new repository on read,
    after that it should be up to date on new updates.
  \item[Maintainability]
    Node administrator can change the storage, and perform data migration from one storage
    to another.
    Repository administrators are able to add or delete node for new region and
    get all nodes status for repository.
  \item[Auditability]
    Node doesn't perform access control operations, but logs all
    requests with identity and performed operation.
  \item[Analytics]
    Node collects statistics for each repository and usage metrics, such as
    push and pull operations, etc. The system keeps the whole statistics about
    nodes, e.g. how many nodes contains each repository, the state of nodes, etc.
\end{description}

\section{Compare to other solutions}

These products are similar to DeGit by some aspects:
\begin{description}
  \item[Spokes]
    GitHub announced \href{https://github.blog/2016-04-05-introducing-dgit/}{DGit}
    in 2016 (renamed to \href{https://github.blog/2016-09-07-building-resilience-in-spokes/}{Spokes})
    where they \href{https://github.blog/2016-09-07-building-resilience-in-spokes/#defining-resilience}{pay attention}
    to the consistency:
    \begin{quote}
      Spokes puts the highest priority on consistency and partition tolerance.
      In worst-case failure scenarios, it will refuse to accept writes that it cannot commit,
      synchronously, to at least two replicas.
    \end{quote}
    It's a proprietary software that can't be used for free and the source code is closed.
    Spokes papers claims that it pays attention to consistency, but on the
    \href{https://www.youtube.com/watch?v=DY0yNRNkYb0}{conference talk} they mentioned that
    it's rarely possible to break the consistency which requires manual intervention.
    Therefore the approach of distributed system design used by Spokes is not suitable for open
    source project, where maintainance team doesn't exist.
  \item[Gitaly]
    \href{https://docs.gitlab.com/ee/README.html}{Gitlab} has
    \href{https://docs.gitlab.com/ee/administration/gitaly/}{Gitaly} service which provides
    \code{gRPC} API for Gitlab website and git-ssh proxy to perform all git operations via API.
    It's \href{https://gitlab.com/gitlab-org/gitaly}{open source} component.
    Gitaly proposed new design for service which claims to provide
    \href{https://gitlab.com/gitlab-org/gitaly/-/blob/master/doc/design\_ha.md\#strong-consistency-design}{strong concistency}
    but in fact it doesn't provide linearability of commands in system \todo{arguments and proves}.
    Futhermore, it's possible that GitLab may change HA licensing \todo{find cases},
    or restrict HA support \href{https://news.ycombinator.com/item?id=21437334}{based on country residence}.
  \item[JGit]
    \href{https://www.eclipse.org/jgit/}{Jgit} is a Java git server created by \href{https://www.eclipse.org/}{Eclipse}.
    Google \href{https://www.eclipse.org//lists/jgit-dev/msg03073.html}{contributed} to this project with Ketch module:
    \begin{quote}
      Git Ketch is a multi-master Git repository management system. Writes
      (such as git push) can be started on any server, at any time. Writes
      are successful only if a majority of participant servers agree.
      Acked writes are durable against server failure, due to a majority of
      the participants storing all required objects.
    \end{quote}
    But this is the only place where Ketch is mentioned. \todo{Analyze source code of Ketch module}.
  \item[IPFS]
    \href{https://ipfs.io/}{IPFS} is not exactly distributed git repository project, but has similar ideas
    and cound be helpfull for us. \todo{analyze IPFS project}.
  \item[brig]
    \todo{analyze the project} \href{https://github.com/sahib/brig}{brig}.
\end{description}

\subsection{Functional Requirements}
\label{sec:nfr}

The most important \href{https://en.wikipedia.org/wiki/Functional_requirement}{functional requirements} are:

\begin{description}
  \item[Front end]
    The system potentically may have different kinds of front-ends,
    but it's required to support \href{https://grpc.io/}{gRPC}
    of \href{https://about.gitlab.com/}{GitLab} to integrate the system
    into GitLab service and replace
    \href{https://docs.gitlab.com/ee/administration/gitaly/}{Gitaly}.
  \item[Back end]
    Each node may be connected to different types of storage for git repos,
    but it's required to support file-system storage.
\end{description}

\subsection{Expected Metrics}
\label{ref:metrics}

In a large enterprise it is expected to have the following
numbers, in terms of load, size, and speed:

\begin{tabular}{ll}
  Repositories & 2M \\
  Active users & 100K/day \\
  Merges & 100K/day \\
  Fetches & 15M/day, 15K/m - peak \\
  Push & 200K/day \\
  Traffic - download & 200Tb/day \\
  Traffic - update & 250Gb/day \\
\end{tabular}

\section{References}
\label{ref:references}

Implementing Fault-Tolerant Services Using the State Machine Approach: A Tutorial, FRED B. SCHNEIDER Department of Computer Science, Cornell University, Ithaca, New York 14853 U.S.A.

Replication Management using the State Machine Approach, Fred B. Schneider Department of Computer Science Cornell University Ithaca, NewYork 14853 U.S.A.

Building resilience in Spokes, Patrick Reynolds, https://github.blog/2016-09-07-building-resilience-in-spokes/

Kademlia: A Peer-to-peer information system based on the XOR Metric, Petar Maymounkov and David Mazieres New Yourk University

S/Kademlia: A Practicable Approach Towards Secure Key-Based RoutingIngmar Baumgart and Sebastian MiesInstitute of TelematicsUniversit at Karlsruhe (TH)D76128 Karlsruhe, Germany

IPFS - Content Addressed, Versioned, P2P File System (DRAFT 3) Juan Benet

\printbibliography
\end{document}
