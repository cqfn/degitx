\documentclass[acmlarge, screen, nonacm, 11pt]{acmart}
\usepackage[utf8]{inputenc}
\usepackage{float}
\usepackage{setspace}
\usepackage{pgfplots}
\usepackage{graphicx}
\usepackage{fancyvrb}
\usepackage{listings}
\usepackage[absolute]{textpos}\TPGrid{16}{16}
\usepackage{tikz}
  \usetikzlibrary{shapes}
  \usetikzlibrary{arrows.meta}
  \usetikzlibrary{arrows}
  \usetikzlibrary{shadows}
  \usetikzlibrary{trees}
  \usetikzlibrary{fit}
  \usetikzlibrary{calc}
  \usetikzlibrary{positioning}
  \usetikzlibrary{decorations.pathmorphing}
\usepackage{./tikz-uml}
\usepackage{xcolor}
\usepackage{hyperref}
  \hypersetup{colorlinks=true,allcolors=blue!40!black}
\setlength{\topskip}{6pt}
\setlength{\parindent}{0pt} % indent first line
\setlength{\parskip}{6pt} % before par
\date{\small\today}
\title[DeGit white paper]{[DRAFT] White-paper: DeGit - distributed git repository manager}
\definecolor{tikz-block}{HTML}{232527}
\tikzset{node distance=1.6cm, auto, every text node part/.style={align=center, font={\sffamily\small}}}
\tikzstyle{block} = [draw=tikz-block, fill=white, inner sep=0.3cm, outer sep=0.1cm, thick]
\tikzstyle{ln} = [draw, ->, very thick, arrows={-triangle 90}, every text node part/.append style={font={\sffamily\scriptsize}}]
\tikzstyle{O} = [circle, draw, every text node part/.style={align=center, font={\sffamily\small}}]

% custom commands
\newcommand{\code}[1]{\texttt{#1}}
\newcommand{\todo}[1]{\textcolor{red}{TODO: #1}}

\author{Kirill Chernyavskiy}
\email{g4s8.public@gmail.com}

\acmBooktitle{none}
\acmConference{none}
\editor{none}

\begin{document}
\raggedbottom

\begin{abstract}
  DeGitX is a distributed git repository manager.
  It provides a front-end interface for git operations by
  exposing one of supported endpoints for git clients, and
  hides the distributed nature of git storage located on back-end nodes.
  The back-end keeps git repository replicas simultaneously on
  multiple different nodes to scale up the read capacity,
  increase durability and provides better availability especially
  for different geographically distributed regions.
\end{abstract}

\maketitle

\section{Introduction}\label{sec:intro}

Providing efficient access to code repositories is a significant challenge
that large software companies or open-source communities may face when
the number of code repositories used by software developers may count
on millions. This challenge becomes especially critical for companies
that have distributed teams around the world.  In this case, using a single
git server does not provide even nearly-appropriate performance.
On the other hand, deploying multiple servers with load balancing
isn’t feasible either because storage I/O scaling is not possible
for reading operations.

This article describes how to achieve significant performance improvement
by configuring a distributed git repository storage that replicates
repositories across nodes. The result can be achieved by implementing
a distributed Git repository manager DeGit with strong-consistent replica
nodes, guaranteeing high availability for read operations,
as well as availability across different regions.
Such a solution also provides durability by replicating repositories
on different server racks and read scalability by routing fetch traffc
to different nodes.
There is a number of technical problems related to scalability,
dispatching, and consistency that need to be solved when implementing
the distributed Git repository manager with strong-consistent replica nodes.
In addition to describing the suggested solution's major work principles
and functionality, this article also addresses these issues.
The DeGit architecture and components, including the main system protocols
that enable the system to identify the node, establish the node's connection,
lookup at the node’s real address, and map a git repository to the
storage node, are also explained.
The potential presence of malicious nodes can misroute, corrupt,
or drop messages and routing information in the system.
This article reviews the methods with which the adopted DeGit approach
protects the system infrastructure from potential attacks.
In this paper, you can find detailed instructions on creating a new repository.
The article also explains how a node finds a replica with a git repository
and provides descriptions of the node discovery workflow and metadata
exchange components.
In this solution, the git communicates with the data storage via a front-end.
The document describes communication between the git and the front-end
and explains how data is passed between the front-end and the repository.
The functional and non-functional requirements for building a distributed
git repository storage and the metrics expected for large enterprises
are addressed further in the document. Readers can also find helpful
information regarding the comparison of DeGit with other solutions.

\section{Technical problems}\label{sec:technical-problems}

There are three main problems to solve for distributed git repository manager (DGRM):
\begin{description}
\item[scalability] --- one repository may be used actively by many users simultaneously. Storage disk
  has input-output (IO) operation limits and can't perform a lot of \code{fetch} operations in
  short period of time. The git repository storage should be replicated on different system nodes to
  solve this problem.
\item[dispatching] --- this user does not know the network addresses of git repository storages. Also, according to the
  previous problem, one repository can be located on multiple replicas.
  So the system should be able to locate and redirect user's requests
  from git client\footnote{Git client is a software that performs git fetch and push operations; it can be
  a command-line tool, IDE or any graphical user interface communicating with DGRM} to correct git repository
  storage. It should lookup for repository nodes in the system and load-balance requests to different storage replicas.
\item[consistency] --- the user may \code{push} to the repository and \code{fetch} then; in that case it must receive
  the same or newer data that user \code{push}ed previously, even if the user \code{fetch}ed data from another
  replica of this repository. The linearizability of every single repository in a system is a must-have option.
\end{description}

\subsection{Scalability}\label{sec:scalability}

Big software companies or large developer communities have
millions of repositories distributed around the world and big teams located in different countries
(regions). On a high load of \code{fetch} requests, git repository storage disks may go above IO operation limits.
Usually, repositories are accessed by different ways:
\begin{description}
  \item[Developer personal activity] --- programmers access git repositories via
    \code{git} client and performs \code{fetch}/\code{push} actions. Also, they
    may use web UI interface to upload files, edit files in browser, merge pull requests, etc.
    The number of requests for this activity is not very large, according to statistics.
  \item[API access] --- many services depend on repository management services: API robots
    collecting developers' activity, background code analyzers of a repository, IDEs communicating
    with repositories and reading some history data. The number of such requests is about
    a few thousand requests per minute for each million repositories.
  \item[CI systems] --- many events may trigger CI workflow; most common events are:
    new commit pushed, new pull (merge) request created, new tag pushed, etc.
    On each action, the CI system downloads the whole repository to run some workflows,
    the download (\code{clone}) operation accompanied by huge bandwidth consumption.
    In addition, some repositories may include submodules; this leads to submodules cloning by the CI
    system. According to the statistic, the number is measured as ten thousand requests per minute
    for each million repositories.
\end{description}

Event if git repository storage is distributed and clients are routed to storage with
correct git repository, the large amount of \code{fetch} traffic for one repository still able to
make disk go above input-output operations (IOP) limits, and repository storage will stop
serving requests.

It was an attempt to fix the scalability problem by asynchronous replication of git repository data:
there was proof of concept DGRM with eventually consistency guarantee;
one repository was replicated asynchronously after any update operation to multiple git repository storages. Thererfore,
clients were load balanced to different git repository storages.
However, it did not succeed because of two reasons:
\begin{enumerate}
  \item fetch traffic correlated with push frequency because of the huge amount of
  CI systems and API robots involved in the development process: each update event usually triggers
  CI build immediately, and CI clones the repository. CI was able to clone only
  primary repository (repository with actual data) because the replication has not been completed
  before CI cloning started.
  \item \code{push} and \code{fetch} frequency was not distributed uniformly over the time --- in each
  repository team members may have different responsibilities for review and merge process,
  project technical lead can merge all approved pull-requests in a short period of time
  which causes frequent push operations in a git repo.
\end{enumerate}

These two conditions lead to high fetch traffic peaks for git repositories:
frequent push operations turn replication storages into an inconsistent state,
and lead to high fetch traffic to primary repository storages from CI systems, which clones these repositories.
It causes the same problems as were common for non-replicated system --- the primary node goes beyond IOPS limits and
rejects new \code{fetcj} requests.

Furthermore, \code{merge} is not the only way to ``update'' the repository. Lots of other activities can do that,
here is the list of some:

\begin{description}
  \item[branches changing] \verb|create|/\verb|delete|/\verb|update| branches by git push or by using web page.
  \item[tags changing] \verb|create|/\verb|delete|/\verb|update| tags by git push or by releasing new version on web page.
  \item[special refs changing] when new merge request is created, a new\\
    \emph{\code{refs/merge-requests/IID\footnote{Internal id of a project on gitlab}/head}} named ref
    is created. When source branch of the merge request is updated, the ref is also updated.
  \item[migrations] sometimes repository administrator can migrate the repository to
    another physical device or another region node; it also could be treated as an update.
    Also, deleting a repository could be treated as migrating it to a trash area.
\end{description}

\subsection{Dispatching}\label{sec:dispatching}

Git client doesn't know where repository replica is located in the system. The dispatching algorithm
should be able to locate correct storage nodes (distributed system nodes) of the repository and redirect
each client's request to one of these nodes. Moreover, it is not enough just to redirect the client's request;
the dispatching endpoint should load-balance all requests for each particular repository to distribute
storage load over time. Round-robin load-balancing solves this problem since the scalability problems
occur only on peeks of high read traffic, so statistically redirecting each next read request to
a different storage node reduces disk load by \code{n}-times, where \code{n} is the number of storage replicas.

\subsection{Consistency}

\todo{Describe the problem}

% \subsection{Government restrictions}
% The most popular repository management systems are \href{https://github.com/}{GitHub} and
% \href{https://about.gitlab.com}{GitLab} are under the control of the government, these services may
% restrict access for some groups of people based on their location and may reject to provide paid
% versions for hosting internal servers. \todo{add more details}.

\section{Solution}\label{sec:solution}

The solution is a distributed Git repository manager with strong
consistent replica nodes.
It consists of two parts: the back-end and front-end
(see Figure~\ref{fig:comp-arc-overview} diagram):

\begin{description}
  \item[The back end] --- (core network, storage), P2P\footnote{peer to peer} system which stores git data
    and metadata\footnote{Here git data referenced to git objects, and git metadata to git references.
    In this document, git data usually refer to both objects and references unless otherwise stated};
    it's responsible for replication of repositories and guarantees
    strong consistency of git storage across replicas. It exposes internal RPC API to accept intermediate git 
    requests, which can modify the repository state and provide endpoints to read (\code{fetch}) repository data.
  \item[The front end] --- (multiplexer, load balancer). It exposes public API for all git operations,
    translates all operations request into intermediate RPC language, routes requests to proper storage
    nodes and load-balancing read (\code{fetch}) requests to different replicas of the same repository.
\end{description}

\begin{figure}
  \begin{center}
    \begin{tikzpicture}
      \umlbasiccomponent[x=-3.5]{Client}

      \begin{umlcomponent}[x=1, y=-4]{Front-end}
        \umlbasiccomponent{Load-balancer}
        \umlprovidedinterface[interface=Entry point, name=lb-e6ce, padding=2cm]{Load-balancer}
        \umlbasiccomponent[y=-3]{Routing table}
      \end{umlcomponent}
      \umlport{Front-end}{west}
      \umldelegateconnector{Front-end-west-port}{lb-e6ce}
      \umlHVHassemblyconnector[interface=RPC, with port, name=fe-rpc,
        middle arm, arm1=-1cm, anchor1=-180]{Client}{Front-end}

      \begin{umlcomponent}[x=6, y=-1]{Back-end}
        \umlbasiccomponent{git storage}
      \end{umlcomponent}
      \umlHVassemblyconnector[interface=RPC, name=be-rpc, with port]{Front-end}{Back-end}

      \umlnote[x=1.2]{fe-rpc-interface}{Front-end exposes public API for git operations}
      \umlnote[x=7, y=-7]{be-rpc-interface}{Back-end exposes internal API for storage operations}
    \end{tikzpicture}
  \end{center}
  \caption{
    Components architecture overview:
    Each client C connected to DeGit front-end component,
    the front-end has load-balancer to proxy incoming requests to back-end,
    it finds back-end nodes using routing table.
  }\label{fig:comp-arc-overview}
\end{figure}

The front end exposes public API for well-known protocol, e.g. one front-end implementation provides
\href{https://docs.gitlab.com/ee/administration/gitaly/}{Gitaly} gRPC interfaces for GitLab
instance set\footnote{In this document GitLab instance set is a set of shell, workhorce and web components of GitLab}
(see Figure ~\ref{fig:gitlab-set} diagram). GitLab is a client of DeGitX front-end, it sends gRPC requests to the
front-end to modify and fetch git data.

\begin{figure}
  \begin{center}
    \begin{tikzpicture}
      \umlbasiccomponent[name=git, y=-0.5]{git CLI}
      \umlbasiccomponent[name=browser, y=-5]{Browser}

      \begin{umlcomponent}[name=gitlab, x=8]{GitLab}
        \umlbasiccomponent[name=gl-shell]{shell}
        \umlprovidedinterface[interface=SSH, name=gl-shell-ssh, padding=2cm, distance=2.5cm]{gl-shell}

        \umlbasiccomponent[name=gl-workhorse, y=-3]{Workhorse}
        \umlprovidedinterface[interface=HTTP, name=gl-wh-http, padding=2cm, distance=2.5cm]{gl-workhorse}

        \umlbasiccomponent[name=gl-web, y=-6]{Web}
        \umlprovidedinterface[interface=HTTP, name=gl-web-http, padding=2cm, distance=2.5cm]{gl-web}
      \end{umlcomponent}
      \umlport{gitlab}{150}
      \umldelegateconnector{gitlab-150-port}{gl-shell-ssh}
      \umlport{gitlab}{210}
      \umldelegateconnector{gitlab-210-port}{gl-wh-http}
      \umldelegateconnector{gitlab-210-port}{gl-web-http}
      \umlassemblyconnector{git}{gitlab-150-port}
      \umlassemblyconnector{git}{gitlab-210-port}
      \umlassemblyconnector{browser}{gitlab-210-port}
      \umlport{gitlab}{east}

      \umlbasiccomponent[name=gitaly-fe, x=8, y=-10]{DeGitX front-end}
      \umlport{gitaly-fe}{east}
      \umlHVHassemblyconnector[interface=Gitaly gRPC, arm1=1cm]{gitlab-east-port}{gitaly-fe-east-port}
    \end{tikzpicture}
  \end{center}
  \caption{
    GitLab set and Gitaly front-end:
    git client communicates with GitLab-shell via SSH or with GitLab-workhorse via HTTP(S).
    The browser uses GitLab-web. All three GitLab components communicate with git storage via DeGitX load balancer
    (front-end) via gRPC protocol defined by Gitaly component of GitLab.
  }\label{fig:gitlab-set}
\end{figure}

The communication layer of DeGit front-end and back-end is DeGit metadata: it consists of mapping
of repository hash to storage node locator. The front-end can be configured differently depends on
setup kind; it can query metadata from the database; it can query lookup it via DHT\footnote{Distributed hash table}
or it can receive metadata updates broadcast from storage node peers in local network via UDP protocol,
see~\ref{sec:metadata}.

When a client (e.g., GitLab-shell) write git data to the system (via \code{push}), the request is being routed
by the front-end load balancer to one of the storage replicas.
The storage back-end node starts leader election with other repository replicas and updates the log\footnote{Here: node log
or back-end log is distributed replicated log of a state machine associated with specific git repository storage}
of repository holders (replicas) consensus, see~\ref{sec:data}.

The system automatically rebalances repository storage: when the repository is not used actively for a
long time, the node can remove a repository from storage if 3 replicas of this repository exist on other nodes;
if some node has a lot of free space on storage, and the storage of another node is almost full,
the node that is full can transfer (move) some repositories to another node. IOPS of storage device is also
a measure of importance.

The system can accept new nodes and automatically fill them with a replica repository and
move some repository to new node. Same for disconnecting: if a node was disconnected from the system or crashed,
it creates additional replicas on nodes to have at least 3 replicas for each repository in a system.

\subsection{Protocol}
Main system protocols are:
\begin{description}
  \item[Location protocol] --- unique node identity.
  \item[Network protocols] --- nodes communication protocols.
  \item[Discovery protocol] --- lookup of a node real address by locator ID.
  \item[Metadata exchange protocol] --- a mapping of git repository to storage back-end node.
  \item[Data exchange protocol] --- Git objects and references exchange protocol,
    commands to add new objects and update references, linearizability guarantee.
\end{description}

\subsubsection{Locators}\label{sec:locators}
Network addresses are not stable; back-end node may get network address via
Dynamic Host Configuration Protocol (DHTP), or node owner may move it from one server to another.
So we can't rely on network addresses when working with back-end nodes; we need some
overlay networks and unique identifiers for each node. To identify a node, DeGit uses
public-key cryptography: node owner generates private and public key pair using one of the supported
crypto algorithms \todo{which exactly?}, these keys will identify back-end node uniquely.
Node uses cryptographic hash \todo{choose algorithm} of the public key as node locator ID,
%FIXME
% as reseearched in the ~\cite{securebaserouting}. Nodes uses locator IDs to introduce
itself to the system and build overlay network on top of real network. DeGit uses
Multihash\footnote{https://multiformats.io/multihash/} format to encode locator IDs.

Private and public keys can be used to sign requests to other nodes or to build
trusted discovery (see~\ref{sec:discovery}) point in a system: when a new system is created,
the administrator can create certification authority (CA) for issuing digital certificates for node
public keys, so each node will be able to verify the certificate of any other node when
using seed list URLs or other peers exchange algorithms. Node instances use locators to talk
with each other. The real network address of nodes can be found using this locator ID.
Also, system uses locators for node-repository mapping (see ~\ref{sec:data}),
the item of mapping table consists of locator ID and unique repository name hash.

\subsubsection{Network}\label{sec:network}

DeGitX uses TCP or UDP protocols for transport layer, TCP is used by default. It uses IPv4 or IPv6 for addressing,
it stores addresses in \href{https://github.com/multiformats/multiaddr}{multiaddr} format, for instance:
\code{/ipv4/1.2.3.4/udp/4444} evaluates to \code{4444} UDP port on \code{1.2.3.4} IPv4 address.
DeGitX uses a routing system (see \ref{sec:discovery}) to find addresses of nodes by locators.

\subsubsection{Discovery}\label{sec:discovery}

Each node doesn't know the network addresses of others by default;
it knows only locator identifier.
Different discovery techniques are used for node lookup.
They can be configured independenly by node
administrator and used together:

\begin{description}
  \item[LPD] --- local peer discovery: each peer sends UDP messages with locator ID.
    It's cheap and fast for the network layer (due to UDP messages); other system components,
    such as front-ends and back-ends receive these messages and update local cache of node locators.
    This approach dramatically improves lookup performance in local networks, e.g. when most of the communications
    are performed in one local region, and all regional nodes are connected via a local network.
    This protocol is supposed to be used with other discovery techniques.
    For security reasons, in untrusted networks, the broadcast messages may be optionally signed with
    node private keys to be verified by the receiver using the CA public certificate.
    For optimization reasons, this protocol is reused by metadata exchange protocol for
    repository hash table propagation (see \ref{sec:metadata}). \todo{lookup for researches}.
    There are two common ways to implement LDP: \href{http://bittorrent.org/beps/bep_0014.html}{bep 14} and \href{http://bittorrent.org/beps/bep_0026.html}{bep 26}.
    Bep 14 is SSDP-like style and Bep 26 is zeroconf style LDP. To implement Bep 26 each host is required to run a zeroconf service discovery daemon.
    There is a popular \href{https://github.com/grandcat/zeroconf}{go zeroconf implementation} that could be used.
    BitTorrent uses following multicast groups: A) \code{239.192.152.143:6771} (\href{https://tools.ietf.org/html/rfc2365#section-6.2}{org-local}) and B) \code{[ff15::efc0:988f]:6771} (\href{https://tools.ietf.org/html/rfc4291#section-2.5.7}{site-local}) for Bep 14 implementation.
    \begin{itemize}
      \item Site-Local scope is intended to span a single site.
      \item Organization-Local scope is intended to span multiple sites
      belonging to a single organization.
    \end{itemize}
    They've chosen such IPs because \code{239.192.0.0/14} is defined to be the IPv4 Organization Local Scope,
    and is the space from which an organization should allocate sub-
    ranges when defining scopes for private use.
    \code{ff15::efc0:988f} also comes from \href{http://tools.ietf.org/html/rfc4291#section-2.7}{IPv6 spec} and means:
    \begin{itemize}
      \item[--] FF == Multicast
      \item[--] 1 == ‘Flags' --- where 1 indicates a nonpermanently assigned
        (``transient'' or ``dynamically'' assigned) multicast address.
      \item[--] 5 == Site-Local scope
      \item[--] efc0:988f --- the hex representation of 239.192.152.143
    \end{itemize}
    We could easily implement BEP 14 as described and take \href{https://github.com/transmission/transmission/blob/7f147c65fb07a6baed3d079703ff0a31d1b1ca4c/libtransmission/tr-lpd.c}{this implementation} as an example.
  \item[Distributed DB] --- network addresses could be propagated to the system using a supplementary distributed
    database, e.g., etcd or others. On startup, the back-end node registers itself in the database,
    and other system nodes (both front-ends and back-ends) use this database for lookups. 
    It requires additional system configuration but delegating some responsibilities to third-party services.
  \item[Seed hosts] --- nodes can use other nodes as seed hosts as caching optimization; some nodes
    will be responsible for caching lookup results. In other words, these seed nodes represent a similar abstraction
    as Content Delivery Network (CDN) in web caching.
  \item[DHT] --- distributed hash tables use different metrics to compare the distance of nodes in the overlay network.
    For example Kademlia\footnote{pdos.csail.mit.edu/~petar/papers/maymounkov-kademlia-lncs.pdf} uses
    \code{XOR} metric of node ID for distance measurement. It requires node IDs to be widely distributed ---
    DeGit locators satisfy these requirements, being generated as cryptographic hash function from
    the public key. In the Kademlia lookup system, each node stores references in K-buckets, where each K-bucket
    contains node addresses with the same ID prefix.
\end{description}

\begin{figure}
  \begin{center}
    \begin{tikzpicture}
      \begin{umlcomponent}{Node}
        \umlbasiccomponent[name=locators]{Locators}
        \umlprovidedinterface[name=ilocators]{locators}

        \umlbasiccomponent[name=discovery, y=-4, x=-3]{Discovery}
        \umlVHVassemblyconnector{discovery}{ilocators}
      \end{umlcomponent}
      \umlport{Node}{355}
      \umlport{Node}{330}
      \umlport{Node}{315}
      \umldelegateconnector{discovery}{Node-355-port}
      \umldelegateconnector{discovery}{Node-330-port}
      \umldelegateconnector{discovery}{Node-315-port}
      \umlrequiredinterface[interface=DHT]{Node-355-port}
      \umlrequiredinterface[interface=LPD]{Node-330-port}

      \umlbasiccomponent[name=db, x=6, y=-5]{Database}
      \umlassemblyconnector[interface=DB]{Node-315-port}{db}
    \end{tikzpicture}
  \end{center}
  \caption{
    Doscovery protocols depends on Locators protocol,
    and uses multiple discovery interfaces: LPD on local network,
    regional database to speedup the lookup, DHT for global lookup
    in all system.
  }\label{fig:discovery-protocol}
\end{figure}

Depends on configuration, the node may use or may not some discovery protocols.
They are performed in well-specified order: firstly,
the node is lookup for locator ID in the local cache; the cache is updated
by LPD broadcasts; in case if not found, the node queries the discovery
database, then seed hosts, and DHT as a last resort.

The structure of discovery entries is a mapping of locator ID
(which is cryptographic hash by design) to network address
in Multiaddr format (as described in network section:~\ref{sec:network}).

Example of the routing table:

\begin{tabular}{l | l}
  Locator ID & Node address \\ \hline
  \code{122041dd7b6443...0022ab11d2589a8} & \code{/ipv4/192.168.1.42/tcp/9031} \\
  \code{122041dd7b6443...0022ab11d2589a8} & \code{/ipv4/192.168.1.33/tcp/8011} \\
  \code{132052eb4dd19f...6f8c7d235eef5f4} & \code{/ipv4/172.18.11.22/tcp/9031} \\
\end{tabular}

\subsection{Metadata}\label{sec:metadata}

DeGit peers don't know where to find repository storage by default.
To solve it, the system introduces metadata layer to exchange repository coordinates
to locator IDs. The structure of metadata is a many-to-many relation of
repository cryptographic hash to storage locator ID.

For instance, here is the example of metadata of two repositories located at two nodes,
repository \code{repo1} is located on both nodes \code{node1} and \code{node2},
the repository \code{repo2} is located only on node \code{node2}:

\begin{tabular}{l | l}
  Repository hash & Locator ID \\ \hline
  \code{hash(repo1)} & \code{locator(node1)} \\
  \code{hash(repo1)} & \code{locator(node2)} \\
  \code{hash(epo2)} & \code{locator(node2)} \\
\end{tabular}

The repository hash is encoded in Multihash format. Metadata exchange protocol partially reuses
discovery protocol for network optimizations, e.g., peers send local broadcasts
with locator IDs and known repository hashes, discovery database may keep (if configured)
repository hash map to node locator IDs relations (see \ref{fig:repo-lookup-db}),
DHT keeps locator IDs as a value for repository hash keys \ref{fig:repo-lookup-dht},
it keeps all metadata of the whole system. The metadata lookup process is similar to the discovery protocol:
firstly, the peer is looking for metadata in the local cache (populated with network broadcasts); then, it checks
the region database; and as a last resort, it performs query lookup for global DHT.

When a new node starts replicating some repository, it's synchronizing with other replicas first, and then
updates metadata asynchronously. The metadata is updated as follows:
\begin{enumerate}
  \item A new node wants to replicate some repository
  \item The node finds current repository holders (replicas) in existing metadata and choose a random node from this list
  \item A node sends the request to the node to add itself to the replica list
  \item Receiver node starts leader election and updates node log to add new node to the replica list
  \item The consensus accepts a new node and stores it in local persistent storage
  \item The leader notifies a new node that it becomes a part of the replicas and consider it when consensus is required
  \item New node replicates the state of consensus and holds the repository and sends UDP broadcast to
    local peers on success
  \item Leader node propagates replicas change in metadata storages asynchronously: updates DHT or database storage
    (as configured)
\end{enumerate}

Repository mapping could be stored in a distributed hash table (e.g., Kademlia),
database cache or broadcasted via local network broadcasts.
Therefore, for multi-region cluster setup, the front-end load balancer may look for a node in the local cache
populated by local network broadcasts, then check region database where all repositories in the same region are
registered, and if not found perform DHT query lookup in different regions,
see Figures \ref{fig:repo-lookup-db} \ref{fig:repo-lookup-dht}.

\begin{figure}
  \begin{center}
    \begin{tikzpicture}
      \node[block] (db) {DB};
      \node[block, below=of db, xshift=-3cm] (fe) {Front-end};
      \node[O, below=of db, xshift=3cm] (Nx) {N$_{x}$};

      \draw[<->] (fe) -- node {1} (db);
      \draw[<->] (fe) -- node {2} (Nx);
    \end{tikzpicture}
  \end{center}
  \caption{
    Repository lookup in database:
    DB --- Database with metadata for current region.
    1 step --- front-end load balancer query database for repository metadata.
    2 step --- front-end access node N$_{x}$ with required repository.
  }\label{fig:repo-lookup-db}
\end{figure}

\begin{figure}
  \begin{center}
    \begin{tikzpicture}
      \matrix[row sep=10mm](feg){
        \node[block] (fe) {Front-end}; \\
      };
      \matrix[row sep=3mm, column sep=5mm, right=of feg](r1){
        \node[O] (r1n1) {N$_{r1,1}$}; \\
        \node[O] (r1n2) {N$_{r1,2}$}; \\
        \node[O] (r1n3) {N$_{r2,3}$}; \\
        \node[O] (r1nN) {N$_{r1,n}$}; \\
      };
      \node[fit=(r1)(feg), draw, dashed, label={[right=0cm, above=3cm]1cm:R$_{1}$}](r1g){};
      \matrix[row sep=3mm, column sep=5mm, right=of r1](r2){
        \node[O] (r2n1) {N$_{r2,1}$}; \\
        \node[O] (r2n2) {N$_{r2,2}$}; \\
        \node[O] (r2nN) {N$_{r2,n}$}; \\
      };
      \node[fit=(r2), draw, dashed, label={[right=0cm, above=3cm]1cm:R$_{2}$}](r2g){};
      \matrix[row sep=3mm, column sep=5mm, right=of r2](r3){
        \node[O] (r3n1) {N$_{r3,1}$}; \\
        \node[O] (r3n2) {N$_{r3,2}$}; \\
        \node[O] (r3n3) {N$_{r3,3}$}; \\
        \node[O] (r3nN) {N$_{r3,n}$}; \\
      };
      \node[fit=(r3), draw, dashed, label={[right=0cm, above=3cm]1cm:R$_{3}$}](r3g){};
      \draw[-latex] (fe) -- (r1n2);
      \draw[-latex] (r1n2) -- (r2n1);
      \draw[-latex] (r2n1) -- (r3n3);
    \end{tikzpicture}
  \end{center}
  \caption{
    Repository lookup via DHT:
    Front-end in Region R$_{1}$ unable to find repository located on node N$_{r3,3}$ in same region,
    but it query the nearest known node N$_{r2,1}$, the node N$_{r2,1}$ knows where repository is located
    and redirects the query to node N$_{r3,3}$ with repository.
  }\label{fig:repo-lookup-dht}
\end{figure}

\subsection{Security}\label{subsec:security}

A small fraction of malicious nodes can prevent correct message delivery throughout the overlay.

Such nodes may misroute, corrupt, or drop messages and routing information.
Additionally, they may attempt to assume the identity of other nodes and corrupt or delete objects
they are supposed to store on behalf of the system.

All attacks are based on the presence of malicious nodes.
There are two ways:
\begin{itemize}
  \item[$-$] Implement techniques that allow nodes to join the overlay, maintain routing state, and forward messages securely in malicious nodes.
  \item[$-$] Make the appearance of malicious nodes impossible.
\end{itemize}

Degitx is not an open peer-to-peer system
where resource pooling without preexisting trusted relationships is possible.
It means that untrusted nodes aren't allowed to join
and all members of the network are trusted not to cheat.

To be sure that the node is trusted, its nodeId certificates should be signed by trusted CAs.
Then each node rejects all unsigned requests.

Certified nodeIds work well when nodes have fixed nodeIds.
This condition is met while Node uses a cryptographic hash of public key as nodeId.

These certificates give the overlay a public key infrastructure
suitable for establishing encrypted and authenticated channels between nodes.
Nodes with valid nodeId certificates can join the overlay, route messages,
and repeatedly leave without the involvement of the CAs.

When the membership of a peer-to-peer system is constraint and all nodes are trusted as in DeGitX (??????),
%FIXME
% CAs could solve security issues, because, according to the ~\cite{securerouting} research, all attacks are based on malicious nodes presence.

\subsection{Git data exchange}
\label{sec:data}
Consensus could be implemented using \emph{\href{https://raft.github.io/raft.pdf}{Raft}} or
\emph{\href{http://www.cs.yale.edu/homes/aspnes/pinewiki/Paxos.html}{Paxos}} algorithm.

Github Spokes and Gitaly HA achieve strong consistency via \emph{\href{https://en.wikipedia.org/wiki/Three-phase_commit_protocol}{3PC protocol}}.
Spokes sources are closed but they mentioned the protocol in their \emph{\href{https://github.blog/2017-10-13-stretching-spokes/\#reducing-round-trips}{blog}}.
So we only know they use it somehow.
On the other hand, Gitaly team, that had faced the same issue, have made open research and a couple of POC to decide how exactly to implement 3PC protocol.
After all experiments:
\emph{\href{https://gitlab.com/gitlab-org/gitaly/-/issues/2466}{Implement 3PC for WriteRef RPC}}
\emph{\href{https://gitlab.com/gitlab-org/gitaly/-/issues/2529}{3PC git-update-ref experiment}}
\emph{\href{https://gitlab.com/gitlab-org/gitaly/-/issues/2635}{2PC via pre-receive hook}},
  they concluded, that the best and only way is to push transaction handling as far down as possible, which is the git client itself and, more specifically, its ref transaction handling.
The only problem is that they need a place in git, where all write operations could be captured, checked and committed or safely aborted.
There was no such place to cover all write operation, and new
\emph{\href{https://github.com/git/git/commit/675415976704459edaf8fb39a176be2be0f403d8}{reference transaction hook}}
was introduced by Gitaly team.
It was designed to implement a voting mechanism and it could be integrated with any consensus algorithm including raft or paxos.
It also provides a lock mechanism on set of refs tha should be updated in current transaction, so it is possible to handle multiple updates to different independent sets of refs inside one repo at the same time.
It means that several branches could be updated parallel without locking each other.

\todo{search for more algorithms}.

\include{explanation}

\section{Requirements}
\label{sec:requirements}

\subsection{Features}
\label{sec:features}

The most critical
\href{https://en.wikipedia.org/wiki/Non-functional_requirement}{Non-functional requirements}
are:

\begin{description}
  \item[Read scalability]
    The solution should scale out the read capacity of a system, each region should be able
    to access repository using most available replica node.
  \item[Strong consistency]
    All? (\todo{discuss, maybe not all but the majority of replicas})
    active replica repositories should be synchronized on updates in any node
    with immediate consistency.
  \item[Durability]
    The system must have enough replicas to recover itself in case of corruption.
    Corrupted repository could be responsible for recovering itself using replica nodes.
  \item[Self management (rename?)]
    Each node performs cleanup when needed (\code{git gc}) and may remove replica
    from storage on read inactivity.
    A node should be able to find and synchronize new repository on read,
    after that it should be up to date on new updates.
  \item[Maintainability]
    Node administrator can change the storage, and perform data migration from one storage
    to another.
    Repository administrators are able to add or delete node for new region and
    get all nodes status for repository.
  \item[Auditability]
    Node doesn't perform access control operations, but logs all
    requests with identity and performed operation.
  \item[Analytics]
    Node collects statistics for each repository and usage metrics, such as
    push and pull operations, etc. The system keeps the whole statistics about
    nodes, e.g.\@ how many nodes contains each repository, the state of nodes, etc.
\end{description}

\section{Compare to other solutions}

These products are similar to DeGit by some aspects:
\begin{description}
  \item[Spokes]
    GitHub announced \href{https://github.blog/2016-04-05-introducing-dgit/}{DGit}
    in 2016 (renamed to \href{https://github.blog/2016-09-07-building-resilience-in-spokes/}{Spokes})
    where they \href{https://github.blog/2016-09-07-building-resilience-in-spokes/#defining-resilience}{pay attention}
    to the consistency:
    \begin{quote}
      Spokes puts the highest priority on consistency and partition tolerance.
      In worst-case failure scenarios, it will refuse to accept writes that it cannot commit,
      synchronously, to at least two replicas.
    \end{quote}
    It's a proprietary software that can't be used free and the source code is closed.
    Spokes papers claims that it pays attention to consistency, but on the
    \href{https://www.youtube.com/watch?v=DY0yNRNkYb0}{conference talk} they mentioned that
    it's rarely possible to break the consistency which requires manual intervention.
    Therefore the approach of distributed system design used by Spokes is not suitable for open
    source project, where maintainance team doesn't exist.
  \item[Gitaly]
    \href{https://docs.gitlab.com/ee/README.html}{Gitlab} has
    \href{https://docs.gitlab.com/ee/administration/gitaly/}{Gitaly} service which provides
    \code{gRPC} API for Gitlab website and git-ssh proxy to perform all git operations via API.
    It's \href{https://gitlab.com/gitlab-org/gitaly}{open source} component.
    Gitaly proposed new design for service which claims to provide
    \href{https://gitlab.com/gitlab-org/gitaly/-/blob/master/doc/design\_ha.md\#strong-consistency-design}{strong concistency}
    but in fact it doesn't provide linearability of commands in system \todo{arguments and proves}.
    Futhermore, it's possible that GitLab may change HA licensing \todo{find cases},
    or restrict HA support \href{https://news.ycombinator.com/item?id=21437334}{based on country residence}.
  \item[JGit]
    \href{https://www.eclipse.org/jgit/}{Jgit} is a Java git server created by \href{https://www.eclipse.org/}{Eclipse}.
    Google \href{https://www.eclipse.org//lists/jgit-dev/msg03073.html}{contributed} to this project with Ketch module:
    \begin{quote}
      Git Ketch is a multi-master Git repository management system. Writes
      (such as git push) can be started on any server, at any time. Writes
      are successful only if a majority of participant servers agree.
      Acked writes are durable against server failure, due to a majority of
      the participants storing all required objects.
    \end{quote}
    But this is the only place where Ketch is mentioned. \todo{Analyze source code of Ketch module}.
  \item[IPFS]
    \href{https://ipfs.io/}{IPFS} is not exactly distributed git repository project, but has similar ideas
    and cound be helpfull for us. \todo{analyze IPFS project}.
  \item[brig]
    \todo{analyze the project} \href{https://github.com/sahib/brig}{brig}.
\end{description}

\subsection{Functional Requirements}
\label{sec:nfr}

The most important \href{https://en.wikipedia.org/wiki/Functional_requirement}{functional requirements} are:

\begin{description}
  \item[Front end]
    The system potentically may have different kinds of front-ends,
    but it's required to support \href{https://grpc.io/}{gRPC}
    of \href{https://about.gitlab.com/}{GitLab} to integrate the system
    into GitLab service and replace
    \href{https://docs.gitlab.com/ee/administration/gitaly/}{Gitaly}.
  \item[Back end]
    Each node may be connected to different types of storage for git repos,
    but it's required to support file-system storage.
\end{description}

\subsection{Expected Metrics}
\label{ref:metrics}

In a large enterprise it is expected to have the following
numbers, in terms of load, size, and speed:

\begin{tabular}{ll}
  Repositories & 2M \\
  Active users & 100K/day \\
  Merges & 100K/day \\
  Fetches & 15M/day, 15K/m -- peak \\
  Push & 200K/day \\
  Traffic -- download & 200Tb/day \\
  Traffic -- update & 250Gb/day \\
\end{tabular}

% \include{appendix-a}

\section{References}
\label{ref:references}

Implementing Fault-Tolerant Services Using the State Machine Approach: A Tutorial, FRED B. SCHNEIDER Department of Computer Science, Cornell University, Ithaca, New York 14853 U.S.A.

Replication Management using the State Machine Approach, Fred B. Schneider Department of Computer Science Cornell University Ithaca, NewYork 14853 U.S.A.

Building resilience in Spokes, Patrick Reynolds, https://github.blog/2016-09-07-building-resilience-in-spokes/

Kademlia: A Peer-to-peer information system based on the XOR Metric, Petar Maymounkov and David Mazieres New Yourk University

S/Kademlia: A Practicable Approach Towards Secure Key-Based RoutingIngmar Baumgart and Sebastian MiesInstitute of TelematicsUniversit at Karlsruhe (TH)\@ D–76128 Karlsruhe, Germany

IPFS -- Content Addressed, Versioned, P2P File System (DRAFT 3) Juan Benet

% \printbibliography
\end{document}
