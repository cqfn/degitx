\documentclass[acmlarge, screen, nonacm, 11pt]{acmart}
\usepackage[utf8]{inputenc}
\usepackage{adjustbox}
\usepackage{float}
\usepackage{setspace}
\usepackage{pgfplots}
\usepackage{graphicx}
\usepackage{fancyvrb}
\usepackage{listings}
\usepackage[absolute]{textpos}\TPGrid{16}{16}
\usepackage{tikz}
  \usetikzlibrary{shapes}
  \usetikzlibrary{arrows.meta}
  \usetikzlibrary{arrows}
  \usetikzlibrary{shadows}
  \usetikzlibrary{trees}
  \usetikzlibrary{fit}
  \usetikzlibrary{calc}
  \usetikzlibrary{positioning}
  \usetikzlibrary{decorations.pathmorphing}
\usepackage{./tikz-uml}
\usepackage{xcolor}
\usepackage{hyperref}
  \hypersetup{colorlinks=true,allcolors=blue!40!black}
\setlength{\topskip}{6pt}
\setlength{\parindent}{0pt} % indent first line
\setlength{\parskip}{6pt} % before par
\date{\small\today}
\title[DeGit white paper]{[DRAFT] White-paper: DeGit - distributed git repository manager}
\definecolor{tikz-block}{HTML}{232527}
\tikzset{node distance=0.6cm, auto, every text node part/.style={align=center, font={\sffamily\small}}}
\tikzstyle{block} = [draw=tikz-block, fill=white, inner sep=0.3cm, outer sep=0.1cm, thick]
\tikzstyle{ln} = [draw, ->, very thick, arrows={-triangle 90}, every text node part/.append style={font={\sffamily\scriptsize}}]
\tikzstyle{O} = [circle, draw, every text node part/.style={align=center, font={\sffamily\small}}]

% custom commands
\newcommand{\code}[1]{\texttt{#1}}
\newcommand{\todo}[1]{\textcolor{red}{TODO: #1}}

\author{Kirill Chernyavskiy}
\email{g4s8.public@gmail.com}

\acmBooktitle{none}
\acmConference{none}
\editor{none}

\begin{document}
\raggedbottom

\begin{abstract}
  DeGitX is a distributed git repository manager.
  It provides a front-end interface for git operations by
  exposing one of the supported endpoints for git clients, and
  hides the distributed nature of git storage located on back-end nodes.
  The back-end keeps git repository replicas simultaneously on
  multiple different nodes to scale up the read capacity,
  increase durability and provides better availability, especially
  for different geographically distributed regions.
\end{abstract}

\maketitle

\section{Introduction}\label{sec:intro}

Providing efficient access to code repositories is a significant challenge
that large software companies or open-source communities may face when
the number of code repositories used by software developers counts
on millions. This challenge becomes especially critical for companies
that have distributed teams around the world. In this case, using a single
git server does not provide even nearly appropriate performance.
On the other hand, deploying multiple servers with load balancing is not feasible
because storage I/O scaling is impossible for reading operations.

This article describes how to achieve significant performance improvement
by configuring a distributed git repository storage that replicates
repositories across nodes. The result can be achieved by implementing
a distributed Git repository manager DeGit with strong-consistent replica
nodes, guaranteeing high availability for reading operations,
as well as availability across different regions.
Such a solution also provides durability by replicating repositories
on different server racks and read scalability by routing fetch traffic
to different nodes.
There is a number of technical problems related to scalability,
dispatching, and consistency, which need to be solved when implementing
the distributed Git repository manager with strong-consistent replica nodes.
In addition to describing the suggested solution's major work principles
and functionality, this article also addresses these issues.
The DeGit architecture and components, including the main system protocols, are also explained.
These enable the system to identify the node, establish the node's connection,
lookup at the node's real address, and map a git repository to the
storage node.
The potential presence of malicious nodes can misroute, corrupt,
or drop messages and routing information in the system.
This article reviews the security-related methods with which the adopted DeGit approach
uses to protect the system infrastructure from potential attacks.
In this paper, you can find detailed instructions on creating a new repository.
The article also explains how a node finds a replica with a git repository
and provides descriptions of the node discovery workflow and metadata
exchange components.
In this solution, the git communicates with the data storage via a front-end.
The document describes communication between the git and the front-end
and explains how data is passed between the front-end and the repository.
The functional and non-functional requirements for building a distributed
git repository storage and the metrics expected for large enterprises
are addressed further in the document. Readers can also find helpful
information regarding the comparison of DeGit with other solutions.

\section{Technical problems}\label{sec:technical-problems}

There are three main problems to solve for the distributed git repository manager (DGRM):
\begin{description}
\item[scalability] --- one repository may be used actively by many users simultaneously. Storage disk
  has input-output (IO) operation limits and cannot perform a lot of \code{fetch} operations in
  a short period of time. The git repository storage should be replicated on different system nodes to
  solve this problem~\cite{scalability}.
\item[dispatching] --- the user does not know the network addresses of git repository storages. Also, according to the
  previous problem, one repository can be located on multiple replicas.
  Hence, the system should be able to locate and redirect the user's requests
  from git client\footnote{Git client is a software that performs git fetch and push operations; it can be
  a command-line tool, IDE or any graphical user interface communicating with DGRM} to correct git repository
  storage. It should lookup repository nodes in the system and load-balance requests to different storage replicas.
\item[consistency] --- the user may \code{push} to the repository and \code{fetch} then; in that case it must receive
  the same or newer data that user \code{push}ed previously, even if the user \code{fetch}ed data from another
  replica of this repository. The linearizability of every single repository in a system is a must-have option.
\end{description}

\subsection{Scalability}\label{sec:scalability}

Big software companies or large developer communities have
millions of repositories distributed worldwide, and big teams located in different countries
(regions). On a high load of \code{fetch} requests, git repository storage disks may go above IO operation limits.
Usually, repositories are accessed in different ways:
\begin{description}
  \item[Developer personal activity] --- programmers access git repositories via
    \code{git} client and performs \code{fetch}/\code{push} actions. They may also use web UI interface to upload files,
    edit files in a browser, merge the pull requests, etc.
    The number of requests for this activity is not very large, according to statistics.
  \item[API access] --- many services depend on repository management services: API robots
    collecting developers' activity, background code analyzers of a repository, IDEs communicating
    with repositories and reading some historical data. The number of such requests is approximately
    a few thousand requests per minute for every million repositories.
  \item[CI systems] --- many events may trigger a CI workflow; most common events are:
    new commit pushed, new pull (merge) request created, new tag pushed, etc.
    On each action, the CI system downloads the whole repository to run some workflows,
    the download (\code{clone}) operation it typically accompanied by huge bandwidth consumption.
    In addition, some repositories may include submodules; this leads to submodules cloning by the CI
    system. According to statistics, the number of such operations is on the scale of ten thousand requests per minute
    for every million repositories.
\end{description}

Even in cases when git repository storage is distributed, and clients are routed to storage with the correct git repository,
a large amount of \code{fetch} traffic for one repository still able to
make disk go above input-output operations (IOP) limits, and repository storage will stop
serving requests.

Asynchronous replication of git repository data was one approach attempting to address such scalability problems;
the DGRM was a proof-of-concept, which aimed to eventually consistently guarantee that one repository was replicated 
asynchronously after any update operation to multiple git repository storages. Therefore,
clients were load balanced to different git repository storages~\cite{Eventual_Consistency}.
However, it did not succeed because of two reasons:

\begin{enumerate}
  \item fetch traffic was correlated with push frequency because of the huge amount of
  CI systems and API robots involved in the development process: each update event usually triggers
  a CI build immediately, and CI clones the repository. CI was able to clone only the
  primary repository (repository with actual data) because the replication has not been completed
  before CI cloning started.
  \item \code{push} and \code{fetch} frequency was not distributed uniformly over time --- in each
  repository team members may have different responsibilities for the review and merge process, while
  the project technical lead can merge all approved pull-requests in a short period of time
  which causes frequent push operations in a git repo.
\end{enumerate}

These two conditions lead to high fetch traffic peaks for git repositories:
Frequent push operations turn replication storages into an inconsistent state,
and lead to high fetch traffic to primary repository storages from CI systems, which clones these repositories.
This causes the same problems as were common for non-replicated system --- the primary node goes beyond IOPS limits and
rejects new \code{fetch} requests.

Furthermore, \code{merge} is not the only way to ``update'' the repository. Many of other activities can do that as well,
such as:

\begin{description}
  \item[branches changing] \verb|create|/\verb|delete|/\verb|update| branches by git push or by using web page.
  \item[tags changing] \verb|create|/\verb|delete|/\verb|update| tags by git push or by releasing new version on web page.
  \item[special refs changing] when new merge request is created, a new\\
    \emph{\code{refs/merge-requests/IID\footnote{Internal id of a project on gitlab}/head}} named ref
    is created. When source branch of the merge request is updated, the ref is also updated.
  \item[migrations] sometimes repository administrator can migrate the repository to
    another physical device or another region node; it also could be treated as an update.
    Also, deleting a repository could be treated as migrating it to a trash area.
\end{description}

\subsection{Dispatching}\label{sec:dispatching}

Git client does not know where a repository replica is located in the system. The dispatching algorithm
should be able to locate the correct storage nodes (distributed system nodes) of the repository and redirect
each client's request to one of these nodes. Moreover, it is not enough just to redirect the client's request.
The dispatching endpoint should load-balance all requests for each particular repository to distribute
storage load over time. Round-robin load-balancing solves this problem since the scalability problems
occur only on peaks of high read traffic. Hence, statistically redirecting each next read request to
a different storage node reduces disk load by \code{n}-times, where \code{n} is the number of storage replicas.

\subsection{Consistency}

\todo{Describe the problem}

% \subsection{Government restrictions}
% The most popular repository management systems \href{https://github.com/}{GitHub} and
% \href{https://about.gitlab.com}{GitLab} are under the control of the government. These services may
% restrict access for some groups of people based on their location and may reject to provide paid
% versions for hosting internal servers. \todo{add more details}.

\section{Solution}\label{sec:solution}

The proposed solution is a distributed Git repository manager with strong
consistent replica nodes.
It consists of two parts: the back-end and front-end
(see Figure~\ref{fig:comp-arc-overview} diagram):

\begin{description}
  \item[The back end] --- (core network, storage), P2P\footnote{peer to peer} system which stores git data
    and metadata\footnote{Here git data referenced to git objects, and git metadata to git references.
    In this document, git data usually refer to both objects and references unless otherwise stated}.
    It is responsible for replication of repositories and guarantees
    strong consistency of git storage across replicas. It exposes internal RPC API to accept intermediate git 
    requests, which can modify the repository state and provide endpoints to read (\code{fetch}) repository data.
  \item[The front end] --- (multiplexer, load balancer). It exposes public API for all git operations,
    translates all operations request into intermediate RPC language, routes requests to proper storage
    nodes and load-balancing read (\code{fetch}) requests to different replicas of the same repository.
\end{description}

\begin{figure}
  \begin{center}
    \begin{tikzpicture}
      \umlbasiccomponent[x=-3.5]{Client}

      \begin{umlcomponent}[x=1, y=-4]{Front-end}
        \umlbasiccomponent{Load-balancer}
        \umlprovidedinterface[interface=Entry point, name=lb-e6ce, padding=2cm]{Load-balancer}
        \umlbasiccomponent[y=-3]{Routing table}
      \end{umlcomponent}
      \umlport{Front-end}{west}
      \umldelegateconnector{Front-end-west-port}{lb-e6ce}
      \umlHVHassemblyconnector[interface=RPC, with port, name=fe-rpc,
        middle arm, arm1=-1cm, anchor1=-180]{Client}{Front-end}

      \begin{umlcomponent}[x=6, y=-1]{Back-end}
        \umlbasiccomponent{git storage}
      \end{umlcomponent}
      \umlHVassemblyconnector[interface=RPC, name=be-rpc, with port]{Front-end}{Back-end}

      \umlnote[x=1.2]{fe-rpc-interface}{Front-end exposes public API for git operations}
      \umlnote[x=7, y=-7]{be-rpc-interface}{Back-end exposes internal API for storage operations}
    \end{tikzpicture}
  \end{center}
  \caption{
    Components architecture overview:
    Each client C connected to DeGit front-end component,
    the front-end has load-balancer to proxy incoming requests to back-en.
    It finds back-end nodes using the routing table.
  }\label{fig:comp-arc-overview}
\end{figure}

The front end exposes public API for well-known protocol, e.g.\ one front-end implementation provides
\href{https://docs.gitlab.com/ee/administration/gitaly/}{Gitaly} gRPC interfaces for GitLab
instance set\footnote{In this document GitLab instance set is a set of Shell, Workhorse and web components of GitLab}
(see Figure~\ref{fig:gitlab-set} diagram). GitLab is a client of DeGitX front-end; it sends gRPC requests to the
front-end to modify and fetch git data.

\begin{figure}
  \begin{center}
    \begin{tikzpicture}
      \umlbasiccomponent[name=git, y=-0.5]{git CLI}
      \umlbasiccomponent[name=browser, y=-5]{Browser}

      \begin{umlcomponent}[name=gitlab, x=8]{GitLab}
        \umlbasiccomponent[name=gl-shell]{shell}
        \umlprovidedinterface[interface=SSH, name=gl-shell-ssh, padding=2cm, distance=2.5cm]{gl-shell}

        \umlbasiccomponent[name=gl-workhorse, y=-3]{Workhorse}
        \umlprovidedinterface[interface=HTTP, name=gl-wh-http, padding=2cm, distance=2.5cm]{gl-workhorse}

        \umlbasiccomponent[name=gl-web, y=-6]{Web}
        \umlprovidedinterface[interface=HTTP, name=gl-web-http, padding=2cm, distance=2.5cm]{gl-web}
      \end{umlcomponent}
      \umlport{gitlab}{150}
      \umldelegateconnector{gitlab-150-port}{gl-shell-ssh}
      \umlport{gitlab}{210}
      \umldelegateconnector{gitlab-210-port}{gl-wh-http}
      \umldelegateconnector{gitlab-210-port}{gl-web-http}
      \umlassemblyconnector{git}{gitlab-150-port}
      \umlassemblyconnector{git}{gitlab-210-port}
      \umlassemblyconnector{browser}{gitlab-210-port}
      \umlport{gitlab}{east}

      \umlbasiccomponent[name=gitaly-fe, x=8, y=-10]{DeGitX front-end}
      \umlport{gitaly-fe}{east}
      \umlHVHassemblyconnector[interface=Gitaly gRPC, arm1=1cm]{gitlab-east-port}{gitaly-fe-east-port}
    \end{tikzpicture}
  \end{center}
  \caption{
    GitLab set and Gitaly front-end:
    git client communicates with GitLab-shell via SSH or with GitLab-workhorse via HTTP(S).
    The browser uses GitLab-web. All three GitLab components communicate with git storage via DeGitX load balancer
    (front-end) via gRPC protocol defined by Gitaly component of GitLab.
  }\label{fig:gitlab-set}
\end{figure}

DeGit metadata is the communication layer of the DeGit front end and back end. It consists of a repository hash to the storage node locator.
The front-end can be configured differently depending on
its setup; it can query metadata from the database; it can support lookup queries via DHT\footnote{Distributed hash table}
or it can receive metadata updates broadcast from storage node peers in local network via UDP protocol,
see~\ref{sec:metadata}.

When a client (e.g., GitLab-shell) writes git data to the system (via \code{push}), the request is routed
by the front-end load balancer to one of the storage replicas.
The storage back-end node starts leader election with other repository replicas and updates the log\footnote{Here: the node log
or back-end log is a distributed replicated log of a state machine associated with specific git repository storage}
of repository holders (replicas) consensus, see~\ref{sec:data}.

The system automatically rebalances repository storage: when the repository is not actively used for a
long time, the node can remove it from storage, as long as 3 replicas of this repository exist on other nodes.
If some node has a lot of free storage space and another node's storage is almost full,
the full node can transfer (move) some repositories to another node. IOPS of a storage device is also
a measure of importance.

The system can accept new nodes, automatically fill them with a replica repository and
move some repository to a new node. The same is true for disconnecting: if a node was disconnected from the system or has crashed,
it creates additional replicas on nodes to have at least 3 replicas for each repository.

\subsection{Protocol}
Main system protocols are:
\begin{description}
  \item[Location protocol] --- unique node identity.
  \item[Network protocols] --- nodes communication protocols.
  \item[Discovery protocol] --- lookup of a node real address by locator ID.
  \item[Metadata exchange protocol] --- mapping of git repository to storage back-end node.
  \item[Data exchange protocol] --- Git objects and references exchange protocol,
    commands to add new objects and update references, linearizability guarantee.
\end{description}

\subsubsection{Locators}\label{sec:locators}
Network addresses are not stable; a back-end node may get its network address via
Dynamic Host Configuration Protocol (DHTP), or node owner, may move it from one server to another.
As a result, we cannot rely on network addresses when working with back-end nodes. Instead, we need
overlay networks and unique identifiers for each node. To identify a node, DeGit uses
public-key cryptography: the node owner generates a private and public key pair using one of the supported
crypto algorithms \todo{which exactly?}. These keys will uniquely identify a back-end node.
A cryptographic hash \todo{choose algorithm} of the public key is used as a node locator ID,
as described in~\cite{S_Kademlia_Secure_Key-Based_Routing}.
Nodes use locator IDs to introduce
themself to the system and build an overlay network on top of the real network. DeGit uses
Multihash\footnote{https://multiformats.io/multihash/} format to encode locator IDs~\cite{IPFS_research}.

Private and public keys can be used to sign requests to other nodes or to build a
trusted discovery (see~\ref{sec:discovery}) point in a system: when a new system is created,
the administrator can create certification authority (CA) for issuing digital certificates for node
public keys, so each node will be able to verify the certificate of any other node when
using seed list URLs or other peers exchange algorithms. Node instances use locators to talk
with each other. The real network address of nodes can be found using this locator ID.
Also, the system uses locators for node-repository mapping (see~\ref{sec:data}), where an
item of mapping table consists of a locator ID and a unique repository name hash.

\subsubsection{Network}\label{sec:network}

DeGitX uses TCP or UDP protocols for transport layer; TCP is used by default. It uses IPv4 or IPv6 for addressing,
storing addresses in \href{https://github.com/multiformats/multiaddr}{multiaddr} format, for instance:
\code{/ipv4/1.2.3.4/udp/4444} evaluates to \code{4444} UDP port on \code{1.2.3.4} IPv4 address.
DeGitX uses a routing system (see~\ref{sec:discovery}) to find addresses of nodes via locators.

\subsubsection{Discovery}\label{sec:discovery}

By default, a node does not know the network addresses of other nodes,
only their locator IDs.
Various discovery techniques are used for node lookup.
They can be configured independently by node
administrator or used together:

\begin{description}
  \item[LPD] --- local peer discovery: each peer sends UDP messages containing the locator ID.
    This technique is cheap and fast for the network layer (due to UDP messages). Other system components,
    such as front-ends and back-ends receive these messages and update a local cache of node locators.
    This approach dramatically improves lookup performance in local networks, e.g. when most of the communications
    are taking place in one local region, and all regional nodes are connected via a local network.
    This protocol is supposed to be used with other discovery techniques.
    For security reasons, in untrusted networks, the broadcast messages may be optionally signed with
    node private keys to be verified by the receiver using the CA public certificate.
    For optimization reasons, this protocol is reused by a metadata exchange protocol for
    repository hash table propagation (see~\ref{sec:metadata}). \todo{lookup for researches}.
    There are two common ways to implement LDP: \href{http://bittorrent.org/beps/bep_0014.html}{bep 14} and \href{http://bittorrent.org/beps/bep_0026.html}{bep 26}.~\cite{bittorrent}. todo:  Please, check this ref. (Check bittorent and zeroconf researches instead of URL links here)!!!!!
    Bep 14 is SSDP-like style and Bep 26 is zeroconf style LDP. To implement Bep 26 each host is required to run a zeroconf service discovery daemon.
    There is a popular \href{https://github.com/grandcat/zeroconf}{go zeroconf implementation} that could be used.
    BitTorrent uses following multicast groups: A) \code{239.192.152.143:6771} (\href{https://tools.ietf.org/html/rfc2365#section-6.2}{org-local}) and B) \code{[ff15::efc0:988f]:6771} (\href{https://tools.ietf.org/html/rfc4291#section-2.5.7}{site-local}) for Bep 14 implementation.
    \begin{itemize}
      \item Site-Local scope is intended to span a single site.
      \item Organization-Local scope is intended to span multiple sites
      belonging to a single organization.
    \end{itemize}
    They've chosen such IPs because \code{239.192.0.0/14} is defined to be the IPv4 Organization Local Scope,
    and is the space from which an organization should allocate sub-
    ranges when defining scopes for private use.
    \code{ff15::efc0:988f} also comes from \href{http://tools.ietf.org/html/rfc4291#section-2.7}{IPv6 spec} and means:
    \begin{itemize}
      \item[--] FF == Multicast
      \item[--] 1 == â€˜Flags' --- where 1 indicates a nonpermanently assigned
        (``transient'' or ``dynamically'' assigned) multicast address.
      \item[--] 5 == Site-Local scope
      \item[--] efc0:988f --- the hex representation of 239.192.152.143
    \end{itemize}
    We could easily implement BEP 14 as described and take \href{https://github.com/transmission/transmission/blob/7f147c65fb07a6baed3d079703ff0a31d1b1ca4c/libtransmission/tr-lpd.c}{this implementation} as an example.
  \item[Distributed DB] --- network addresses could be propagated to the system using a supplementary distributed
    database, e.g., etcd or others. On startup, the back-end node registers itself in the database,
    and other system nodes (both front-ends and back-ends) use this database for lookups. 
    It requires additional system configuration but delegates some responsibilities to third-party services~\cite{distributed_key-value_database}.
  \item[Seed hosts] --- nodes can use other nodes as seed hosts for caching optimization; some nodes
    will be responsible for caching lookup results. In other words, these seed nodes represent a similar abstraction
    to Content Delivery Network (CDN) in web caching.
  \item[DHT] --- distributed hash tables use different metrics to compare the distance of nodes in the overlay network.
    For example Kademlia\footnote{pdos.csail.mit.edu/~petar/papers/maymounkov-kademlia-lncs.pdf} uses the
    \code{XOR} metric of node ID for distance measurement. It requires node IDs to be widely distributed ---
    DeGit locators satisfy these requirements, being generated as a cryptographic hash function from
    the public key. In the Kademlia lookup system, each node stores references in K-buckets, where each K-bucket
    contains node addresses with the same ID prefix~\cite{Kademlia_and_Chord_algorithm}.
\end{description}

\begin{figure}
  \begin{center}
    \begin{tikzpicture}
      \begin{umlcomponent}{Node}
        \umlbasiccomponent[name=locators]{Locators}
        \umlprovidedinterface[name=ilocators]{locators}

        \umlbasiccomponent[name=discovery, y=-4, x=-3]{Discovery}
        \umlVHVassemblyconnector{discovery}{ilocators}
      \end{umlcomponent}
      \umlport{Node}{355}
      \umlport{Node}{330}
      \umlport{Node}{315}
      \umldelegateconnector{discovery}{Node-355-port}
      \umldelegateconnector{discovery}{Node-330-port}
      \umldelegateconnector{discovery}{Node-315-port}
      \umlrequiredinterface[interface=DHT]{Node-355-port}
      \umlrequiredinterface[interface=LPD]{Node-330-port}

      \umlbasiccomponent[name=db, x=6, y=-5]{Database}
      \umlassemblyconnector[interface=DB]{Node-315-port}{db}
    \end{tikzpicture}
  \end{center}
  \caption{
    Doscovery protocols depend on Locators protocol,
    and uses multiple discovery interfaces: LPD on local network,
    regional database to speedup the lookup, DHT for global lookup
    in all system.
  }\label{fig:discovery-protocol}
\end{figure}

Depending on the configuration, the node may or may not use some discovery protocols.
They are performed in a well-specified order: first,
the node performs a lookup for locator ID in the local cache; the cache is updated
by LPD broadcasts; in case if it is not found, the node queries the discovery
database, then seed hosts, and DHT as a last resort.

The structure of discovery entries is a mapping of locator ID
(which is cryptographic hash by design) to network address
in Multiaddr format (as described in network section:~\ref{sec:network}).

Example of a routing table:

\begin{tabular}{l | l}
  Locator ID & Node address \\ \hline
  \code{122041dd7b6443...0022ab11d2589a8} & \code{/ipv4/192.168.1.42/tcp/9031} \\
  \code{122041dd7b6443...0022ab11d2589a8} & \code{/ipv4/192.168.1.33/tcp/8011} \\
  \code{132052eb4dd19f...6f8c7d235eef5f4} & \code{/ipv4/172.18.11.22/tcp/9031} \\
\end{tabular}

When a new node starts, it requires discovery protocols to be configured,
depends on configuration, it starts corresponding services to register itself in a system,
then other nodes will be able to find a new node using this registration information. In case
of distributed database, the node just puts new value with a network address for node locator ID
as a key.

\subsection{Metadata}\label{sec:metadata}

DeGit peers don't know where to find repository storage by default.
The system introduces metadata layer to exchange repository coordinates
to locator IDs. The structure of metadata is a many-to-many relation of
repository cryptographic hash to storage locator ID.

For instance, below is an example of metadata of two repositories located at two nodes:
repository \code{repo1} is located on both nodes \code{node1} and \code{node2}, while
repository \code{repo2} is located only on node \code{node2}:

\begin{tabular}{l | l}
  Repository hash & Locator ID \\ \hline
  \code{hash(repo1)} & \code{locator(node1)} \\
  \code{hash(repo1)} & \code{locator(node2)} \\
  \code{hash(epo2)} & \code{locator(node2)} \\
\end{tabular}

The repository hash is encoded in Multihash format. Metadata exchange protocol partially reuses
discovery protocol for network optimizations. Peers send local broadcasts
with locator IDs and known repository hashes, and the discovery database may keep (if configured)
the repository hash map to node locator ID relations (see~\ref{fig:repo-lookup-db}).
DHT keeps locator IDs as a value for repository hash keys~\ref{fig:repo-lookup-dht};
it keeps all metadata of the whole system (???). The metadata lookup process is similar to the discovery protocol:
first, the peer looks for metadata in the local cache (populated with network broadcasts); then, it checks
the region database. Finally, as a last resort, it performs a query lookup for global DHT.

When a new node starts replicating some repository, it first synchronizes with other replicas, and then
updates metadata asynchronously. The metadata is updated as follows:
\begin{enumerate}
  \item A new node wants to replicate some repository
  \item The node finds current repository holders (replicas) in existing metadata and choose a random node from this list
  \item The node sends a request to the selected node to add itself to the replica list
  \item Receiver node starts leader election and updates node log to add a new node to the replica list
  \item The consensus accepts new node and stores it in local persistent storage
  \item The leader notifies a new node that it becomes a part of the replicas and consider it when consensus is required
  \item New node replicates the state of consensus, holds the repository and sends UDP broadcast to
    local peers on success
  \item Leader node propagates replicas change in metadata storages asynchronously, updating DHT or database storage
    (as configured)
\end{enumerate}

Repository mapping could be stored in a distributed hash table (e.g., Kademlia),
database cache, or broadcasted via local network broadcasts.
Therefore, for multi-region cluster setup, the front-end load balancer may look for a node in the local cache
populated by local network broadcasts, then check region database where all repositories in the same region are
registered, and if not found perform DHT query lookup in different regions,
see Figures~\ref{fig:repo-lookup-db} and~\ref{fig:repo-lookup-dht}.

\begin{figure}
  \begin{center}
    \begin{tikzpicture}
      \node[block] (db) {DB};
      \node[block, below=of db, xshift=-3cm] (fe) {Front-end};
      \node[O, below=of db, xshift=3cm] (Nx) {N$_{x}$};

      \draw[<->] (fe) -- node {1} (db);
      \draw[<->] (fe) -- node {2} (Nx);
    \end{tikzpicture}
  \end{center}
  \caption{
    Repository lookup in database:
    DB --- Database with metadata for current region.
    1 step --- front-end load balancer query database for repository metadata.
    2 step --- front-end access node N$_{x}$ with the required repository.
  }\label{fig:repo-lookup-db}
\end{figure}

\begin{figure}
  \begin{center}
    \begin{tikzpicture}
      \matrix[row sep=10mm](feg){
        \node[block] (fe) {Front-end}; \\
      };
      \matrix[row sep=3mm, column sep=5mm, right=of feg](r1){
        \node[O] (r1n1) {N$_{r1,1}$}; \\
        \node[O] (r1n2) {N$_{r1,2}$}; \\
        \node[O] (r1n3) {N$_{r2,3}$}; \\
        \node[O] (r1nN) {N$_{r1,n}$}; \\
      };
      \node[fit=(r1)(feg), draw, dashed, label={[right=0cm, above=3cm]1cm:R$_{1}$}](r1g){};
      \matrix[row sep=3mm, column sep=5mm, right=of r1](r2){
        \node[O] (r2n1) {N$_{r2,1}$}; \\
        \node[O] (r2n2) {N$_{r2,2}$}; \\
        \node[O] (r2nN) {N$_{r2,n}$}; \\
      };
      \node[fit=(r2), draw, dashed, label={[right=0cm, above=3cm]1cm:R$_{2}$}](r2g){};
      \matrix[row sep=3mm, column sep=5mm, right=of r2](r3){
        \node[O] (r3n1) {N$_{r3,1}$}; \\
        \node[O] (r3n2) {N$_{r3,2}$}; \\
        \node[O] (r3n3) {N$_{r3,3}$}; \\
        \node[O] (r3nN) {N$_{r3,n}$}; \\
      };
      \node[fit=(r3), draw, dashed, label={[right=0cm, above=3cm]1cm:R$_{3}$}](r3g){};
      \draw[-latex] (fe) -- (r1n2);
      \draw[-latex] (r1n2) -- (r2n1);
      \draw[-latex] (r2n1) -- (r3n3);
    \end{tikzpicture}
  \end{center}
  \caption{
    Repository lookup via DHT:
    When a front-end in Region R$_{1}$ is unable to find repository located on node N$_{r3,3}$ in same region,
    it queries the nearest known node N$_{r2,1}$. The node N$_{r2,1}$ knows where repository is located
    and redirects the query to node N$_{r3,3}$ with repository.
  }\label{fig:repo-lookup-dht}
\end{figure}

\subsection{Security}\label{subsec:security}

A small fraction of malicious nodes can prevent correct message delivery throughout the overlay.

Such nodes may misroute, corrupt, or drop messages and routing information.
Additionally, they may attempt to assume other nodes' identity and corrupt or delete objects
they are supposed to store on behalf of the system.

All attacks are based on the presence of malicious nodes.
There are two ways:
\begin{itemize}
  \item[$-$] Implement techniques that allow nodes to join the overlay,
    maintain routing state, and forward messages securely in malicious nodes.
  \item[$-$] Make the appearance of malicious nodes impossible.
\end{itemize}
Degitx is not an open peer-to-peer system
where resource pooling without preexisting trusted relationships is possible.

nodes aren't allowed to join,
and all network members are trusted not to cheat.

To be sure that the node is trusted, its nodeId certificates should be signed by trusted CAs.
Then each node rejects all unsigned requests.

Certified nodeIds work well when nodes have fixed nodeIds.
This condition is met while the node uses a cryptographic hash of public key as nodeId.

These certificates give the overlay a public key infrastructure
suitable for establishing encrypted and authenticated channels between nodes.
Nodes with valid nodeId certificates can join the overlay, route messages,
and repeatedly leave without the involvement of the CAs.

When the membership of a peer-to-peer system is a constraint and all nodes are trusted as in DeGitX (??????),
CAs could solve security issues since according to the~\cite{Secure_routing_peer-to-peer_overlay_networks}, all attacks are based on the presence of malicious nodes.

\subsection{Git data exchange}\label{sec:data}

Github Spokes and Gitaly HA achieve strong consistency via \emph{\href{https://en.wikipedia.org/wiki/Three-phase_commit_protocol}{3PC protocol}}.
Spokes sources are closed, but they mentioned the protocol in their \emph{\href{https://github.blog/2017-10-13-stretching-spokes/\#reducing-round-trips}{blog}}.
So we only know they use it somehow.
On the other hand, Gitaly team, which had faced the same issue, has performed open research and a couple of POC to decide how to implement 3PC protocol.
After all experiments:
\emph{\href{https://gitlab.com/gitlab-org/gitaly/-/issues/2466}{Implement 3PC for WriteRef RPC}}
\emph{\href{https://gitlab.com/gitlab-org/gitaly/-/issues/2529}{3PC git-update-ref experiment}}
\emph{\href{https://gitlab.com/gitlab-org/gitaly/-/issues/2635}{2PC via pre-receive hook}},
  they concluded, that the best and only way is to push transaction handling as far down as possible, which is the git client itself and, more specifically, its ref transaction handling.
The only problem is that they need a place in git, where all write operations could be captured, checked and committed or safely aborted.
There was no such place to cover all write operation, and new
\emph{\href{https://github.com/git/git/commit/675415976704459edaf8fb39a176be2be0f403d8}{reference transaction hook}}
was introduced by the Gitaly team.

The git itself has an internal transaction mechanism and hooks to manage it, the workflow for git transaction is:
\begin{enumerate}
  \item Git receives a pack to apply
  \item Git begins a new transaction and locks references for new update by writing to a lock-file
  \item Git performs checks that the pack could be applied by validatin it and comparing locked references ---
    new reference-update should fit well into git tree
  \item In case if validation passed successfully, git calls reference-transaction hook with \code{prepared}
    message and passes references hash-sums as parameters; if the pack could not be applied, then git
    calls this hook with \code{aborted} message.
  \item If hook exited successfully after prepare (with \code{0} exit code), then git commit the transaction,
    calls reference transaction hook with \code{commited} message unlock references by removing lock-file; in other case,
    if the hook decided to exit with error (using non-zero exit code), then git aborts the transaction,
    removes prepared changes, calls hook with \code{aborted} message and unlock the reference.
\end{enumerate}

This workflow allows to control the transaction flow by introducing transaction manager (TM):
we can handle \code{prepared} ad \code{aborted} states and send it to the TM, then wait for response with a
decision, if the TM decides to commit the prepared transaction, the we continue prepared transaction hook
and exit it with zero status, otherwise, if the TM decides to abort the transaction, we exit from prepared state
with error code making the local git transaction aborted.

This scenario fits into districuted atomic commit protocol, where we run git transaction hook on each replica back-ends
and collect votes (prepared or aborted messages) on the front-end: the front-end node acts as a transaction manager (TM)
in this scheme and back-end replicas as a resource managers (RM). The workflow of the distributed transaction will be:
\begin{enumerate}
    \item The front-end sends a git-pack to all back-end storages
    \item The git repository on each back-end node calls reference-transaction hook, sends the vote to the RM
      of the node, and waits until a signal from the RM
    \item The RM module on each back-end node broadcasts the vote from the hook to the TM
    \item If all RMs are prepared (TM receives prepared votes from each RM), then the TM decides to commit,
      and sends commit message to each RM. Otherwise, if any of the RM aborted (TM receives at least one aborted vote),
      then it decides to abort the transaction and sends abort message to each RM
    \item Each RM receives the decision from TM. If the decision is \code{commit}, then RM signals to reference-transaction
      hook to continue; if the decision is \code{abort}, then the RM signals to transaction hook to exit with error code.
    \item If hook continued with zero-code, then git applies the transaction and makes it visible to git repository;
      If it exited with error code, then git revert this transaction. In any case, git unlock the reference
\end{enumerate}

\emph{NOTE: git repository may contain other git hooks, such as \code{pre-receive}, \code{post-receive}, etc.
These hooks may contains some code which should be performed only once, e.g. sending a email. It means that
only one replica node should execute this hook, so the front-end node should send a flag to one random node
to make it hooks-executable. The hooks-executable node runs all git hooks in proper order, these hooks
may affect reference transaction status, e.g. if \code{pre-receive} hook fails, then the transaction will be aborted,
so hooks status affects the whole transaction, since if hooks-executable node fails, then the TM decides to abort.}

To make the system fault-tolerant, we need to have secondary TMs and distributed voting system for RMs.
Paxos-commit~\cite{consensus-on-transaction-commit} solves this problem by introducing Paxos-simple
consensus algorithm for RMs voting: each RM has a ``proposer'' module to broadcast the vote to ``acceptors'',
making the vote available after the crash of RM or TM. For optimization reasons, the ``acceptor''s could be
deployed to the same back-end nodes as RMs.

The successfully transaction flow could be described in 14 steps, see~\ref{fig:transaction-flow-diagram}
and~\ref{fig:transaction-seq-diagram} for visualisation. These diagrams includes one actor (e.g. Gitlab
workhorse or shell) --- somebody who's sending git pack to the DeGitX front-end; two deployments:
DeGitX front-end and back-end nodes, the transaction may be performed on different deployment's scale,
but it's recommended to have one primary front-end for actor communication, one secondary to perform
transaction management in case of primary failure, and three back-ends for resource management.
The front-end deployment consist of two primary modules:
\begin{itemize}
  \item[LB] Load balancer redirects the traffic for git pack uploading to back-end nodes' endpoints,
    chooses hooks executable node.
  \item[TM] Transaction manager resonsible for managing the transaction state, collect votes from
    resource managers and make decision when to commit or abort the transaction.
\end{itemize}
Back-end deployment has five primary components related to transaction handling:
\begin{itemize}
  \item[git] git processor, it could be separate git executable process or gitlib library.
  \item[hook] git reference transaction hook trigered at reference transaction state changes,
    responsible for communication with RM and waiting for signals from RM to continue or abort the
    transaction.
  \item[RM] Resource manager receives transaction notification updates from hook, uses
    Paxos-commit protocol to broadcast git transaction changes to acceptors, begin the transaction
    using TM, handle transaction commands from TM to commit or abort the transaction,
    notify TM on finish.
  \item[Proposer] part of Paxos-commit protocol, propose RM votes to all acceptors
    in transaction scope, broadcasts the vote to acceptors on other nodes. Each RM has
    it's own Paxos-instance embedded into transaction scope. Proposer on each back-end node
    propagates votes only for associated RM.
  \item[Acceptor] each back-end node manages a set of acceptors for each Paxos instance in
    scope of current transaction. E.g. if a transaction has 3 Paxos instances (3 RM with associated
    proposers), then each back-end node has 3 acceptors for each instance.
\end{itemize}

The flow steps are:
\begin{enumerate}
    %1
    \item Actor sends git pack to the front-end, the request is handled by load balancer.
    %2
    \item Load balancer gets back-end nodes for git repository in the request, chooses
      hooks executable node, and sends the request to each node. The front-end attaches the metadata
      related to the transaction scope, such as acceptor addresses and secondary TM nodes.
    %3
    \item Back end node receives git pack and pass it to git component to process it.
      Git applies changes and starts a new transaction, if references could be updated,
      then git changing the transaction state into \code{prepared}, locks references using lock-file,
      and calls transaction-hook with \code{prepared} message and references to be updated.
    %4
    \item Hook uses reference hash-sums as transaction ID, send it to RM as a vote, and blocks
      git execution until the signal from the RM.
    %5
    \item RM uses the vote from hook to broadcast it to all Paxos instances in this transaction.
      It asks a proposer to distribute a vote value to all acceptors.
    %6
    \item Proposer connects to all acceptors and perform Paxos-simple flow starting with 2A message
      to accept a vote as value (first stage is not needed at the beginning, since we always have
      the only proposer for Paxos instance).
    %7
    \item When the value successfully proposed, the proposer notifies the RM.
    %8
    \item RM sync votes state with the acceptors for each Paxos instance located at the same node,
      and gets the voting table for transaction voting (with RM id as rows and acceptor id as columns).
    %9
    \item RM begins a transaction on TM, since it can't be sure that transaction for this particular
      transaction ID was started alreay --- TM doesn't know the transaction ID prior to first \code{begin}
      call from RM. RM attaches voting table into the begin-transaction message to update transaction state
      on TM. If the front attaches a secondary TM node, then RM duplicates this message to each secondary TM.
    %10
    \item TM receives a begin call. If it's a first message received for this transaction ID, it starts a transaction
      and saves voting table received from RM as initial transaction state. It waits for other RMs to send the
      begin call, unti the table will be full enough: if TM has a quorum of ``prepared'' votes for each RM,
      then TM sends a ``commit'' message to each RM; If TM finds in table that at least one RM has a quorum
      of ``abort'' votes, then it sends an ``abort'' message to each RM.
    %11
    \item RM receives the TM decision: either ``commit'' or ``abort'' and notifies git reference-transaction
      hook with ``continue'' or ``error'' signals repspectively.
    %12
    \item If hook receives ``continue'' signal from RM, it continue execution and exit from prepare state
      with success zero status, then git commit the reference-transaction;
      in case of ``error'' signal, hook exits with error code and git aborts the transaction.
    %13
    \item Git commits or abort the reference transaction locally and call the hook on complete with
      \code{commited} or \code{aborted} repspectively.
    %14
    \item Hook notifies RM that transaction finished and immediately exit
    %15
    \item RM notifies TM that the transaction was finished.
    %16
    \item TM waits for all RM to finish transaction, and sends the response to the actor with success or error
      code based on final transaction decision (commited or aborted).
\end{enumerate}

\begin{figure}
  \begin{center}
    \begin{tikzpicture}[
        wide-block/.style={rectangle, draw=black, fill=white, thick, minimum width=4.7cm, minimum height=1.2cm},
        half-wide-block/.style={rectangle, draw=black, fill=white, thick, minimum width=2cm, minimum height=1.2cm},
        group-node/.style={rectangle, draw=gray, inner sep=0.6cm}
      ]
      %    FE            BE
      % [  lb  ]    [git] [hook]
      % [  tm  ]    [    rm    ]
      %             [prop] [acc]

      % fron-end node
      \node[wide-block]         (lb)                            {Load balancer};
      \node[wide-block]         (tm)    [below=of lb]           {Transaction manager};
      % back-end node
      \node[half-wide-block]    (git)   [right=of lb, xshift=1.5cm] {git};
      \node[half-wide-block]    (hook)  [right=of git]          {hook};
      \node[wide-block]         (rm)    [right=of tm, xshift=1.5cm] {Resource manager};
      \node[half-wide-block]    (prop)  [below=of rm, xshift=-1.35cm] {Proposer};
      \node[half-wide-block]    (acc)   [right=of prop]         {Acceptor};
      \node[group-node]         (fe)    [fit=(lb)(tm), label={Front-ends}] {};
      \node[group-node]         (be)    [fit=(git)(hook)(rm)(prop)(acc), label={Back-ends}] {};
      % flow
      \draw[->]         ($(lb.west)+(-1.6,0)$)          -- node[above] {1}      (lb.west);
      \draw[->]         (lb)                            -- node {2}             (git);
      % \draw[->]         ($(git.east)+(0,0.2)$)          -- node[above] {3}      ($(hook.west)+(0,0.2)$);
      \draw[->]         ($(git.north)+(0,0)$)          ..
            controls ($(git.north)+(0.7,0.7)$) and ($(hook.north)+(-0.7,0.7)$)
                                                        .. node[below] {3}      ($(hook.north)+(0,0)$);
      \draw[->]         ($(hook.south)+(-0.7,0)$)       -- node[right] {4}      ++(0,-0.6);
      \draw[->]         ($(rm.south)+(-1.5,0)$)         -- node[left] {5}       ++(0,-0.6);
      \draw[<->]        (prop.east)                     -- node[above] {6}      (acc.west);
      \draw[->]         ($(rm.south)+(-1.1,-0.6)$)      -- node[right] {7}      ($(rm.south)+(-1.1,0)$);
      \draw[<->]        ($(rm.south)+(1.3,0)$)          -- node[right] {8}      ++(0,-0.6);
      \draw[->]         ($(rm.west)+(0,-0.3)$)          ..
            controls ($(rm.west)+(-0.7,-0.7)$) and ($(tm.east)+(0.7,-0.7)$)
                                                        .. node[above] {9}      ($(tm.east)+(0,-0.3)$);
      \draw[->]         (tm.east)                       -- node[above] {10}     (rm.west);
      \draw[->]         ($(hook.south)+(0,-0.6)$)       -- node[right] {11}     ($(hook.south)+(0,0)$);
      \draw[->]         ($(hook.west)+(0,0.2)$)         -- node[above] {12}     ($(git.east)+(0,0.2)$);
      \draw[->]         ($(git.east)+(0,-0.2)$)         -- node[below] {13}     ($(hook.west)+(0,-0.2)$);
      \draw[->]         ($(hook.south)+(0.7,0)$)        -- node[right] {14}     ++(0,-0.6);
      \draw[->]         ($(rm.west)+(0,0.3)$)           ..
            controls ($(rm.west)+(-0.7,0.7)$) and ($(tm.east)+(0.7,0.7)$)
                                                        .. node[above] {15}     ($(tm.east)+(0,0.3)$);
      \draw[->]         (tm.west)                       -- node[above] {16}     ($(tm.west)+(-1.6,0)$);
    \end{tikzpicture}
  \end{center}
  \caption{
  }\label{fig:transaction-flow-diagram}
\end{figure}


\begin{figure}
  \begin{center}
    \adjustbox{scale=1,center}{%
      \begin{tikzpicture}
        \begin{umlseqdiag}
          \umlactor[no ddots]{CLI}
          \umlobject[no ddots, x=2]{LB}
          \umlobject[no ddots, x=4]{TM}
          \umlobject[no ddots, x=6]{git}
          \umlobject[no ddots, x=8]{hook}
          \umlobject[no ddots, x=10]{RM}
          \umlobject[no ddots, x=12]{Proposer}
          \umlobject[no ddots, x=14]{Acceptor}
          \begin{umlcall}[type=asynchron, op=1]{CLI}{LB}
            \begin{umlcall}[op=2, return={2'}]{LB}{git}
            \end{umlcall}
            \begin{umlcall}[op=3, return=12]{git}{hook}
              \begin{umlcall}[op=4, type=asynchron]{hook}{RM}
                \begin{umlcall}[op=5, type=asynchron]{RM}{Proposer}
                  \begin{umlcall}[op=6]{Proposer}{Acceptor}
                  \end{umlcall}
                  \begin{umlcall}[op=7, type=asynchron]{Proposer}{RM}
                  \end{umlcall}
                \end{umlcall}
                \begin{umlcall}[op=8, dt=5, return={8'}]{RM}{Acceptor}
                \end{umlcall}
                \begin{umlcall}[op=9, type=asynchron]{RM}{TM}
                \end{umlcall}
                \begin{umlcall}[op=10, type=asynchron]{TM}{RM}
                \end{umlcall}
                \begin{umlcall}[op=11, type=asynchron]{RM}{hook}
                \end{umlcall}
              \end{umlcall}
            \end{umlcall}
            \begin{umlcall}[op=13, return={13'}]{git}{hook}
              \begin{umlcall}[op=14, type=asynchron]{hook}{RM}
              \end{umlcall}
            \end{umlcall}
            \begin{umlcall}[op=15, type=asynchron, dt=7]{RM}{TM}
            \end{umlcall}
            \begin{umlcall}[op=16, type=asynchron]{TM}{CLI}
            \end{umlcall}
          \end{umlcall}
        \end{umlseqdiag}
      \end{tikzpicture}
    }
  \end{center}
  \caption{
    Transaction commit sequence diagram
  }\label{fig:transaction-seq-diagram}
\end{figure}

\include{explanation}

\section{Requirements}\label{sec:requirements}

\subsection{Features}\label{sec:features}

The most critical
\href{https://en.wikipedia.org/wiki/Non-functional_requirement}{Non-functional requirements}
are:

\begin{description}
  \item[Read scalability]
    The solution should scale-out the read capacity of a system; each region should be able
    to access the repository using the most available replica node.
  \item[Strong consistency]
    All? (\todo{discuss, maybe not all but the majority of replicas})
    active replica repositories should be synchronized on updates in any node
    with immediate consistency.
  \item[Durability]
    The system must have enough replicas to recover itself in case of corruption.
    The corrupted repository could be responsible for recovering itself using replica nodes.
  \item[Self-management (rename?)]
    Each node performs cleanup when needed (\code{git gc}) and may remove replica
    from storage on reading inactivity.
    A node should be able to find and synchronize new repository on reading. After that, it should be up to date on new updates.
  \item[Maintainability]
    Node administrator can change the storage, and perform data migration from one storage
    to another.
    Repository administrators can add or delete a node for the new region and
    get status of all nodes in the repository.
  \item[Auditability]
    Node doesn't perform access control operations but logs all
    requests with identity and performed operation.
  \item[Analytics]
    Node collects statistics for each repository and usage metrics, such as
    Push and Pull operations, etc. The system keeps the whole statistics about
    nodes, e.g.\@ how many nodes are contained in each repository, the state of nodes, etc.
\end{description}

\subsection{Load balancing}\label{sec:load-balancing}

The front-end decides how to redirect incoming fetch requests to replica nodes.
It knows network addresses and current availability of repository replicas,
so it can decide how to route next fetch request to git repository back-end storage node.
On read access to the repository, it sends a health check message to replica nodes to verify
which are available. It stores the result in local cache with configurable time to live
properties. When the front-end cannot access some of the back-end nodes, it marks it as unavailable
and forgets about it for some time. Then in check availability periodically.
It uses ``Round robin'' load balancing for fetch requests since it provides a uniformly distributed
load for repository nodes on high load peaks.~\todo{REF find references and proves about round-robin
load-balancing hor high load network traffic and correlation with uniform distribution of storage access ---??????--- I was unable to find this topic!}.
Due to the strong consistency of back-end nodes provided by the data-exchange protocol,
it has linearizability property and always provides the latest git data (blobs) and metadata (references).
If any node crashes or becomes down, the front-end won't be able to update the repository. However, it still can read it from live replicas, which means that all repositories located on this
node become read-only on node failure.

\section{Compare to other solutions}\label{sec:compare}

These products are similar to DeGit by some aspects:
\begin{description}
  \item[Spokes]
    GitHub announced \href{https://github.blog/2016-04-05-introducing-dgit/}{DGit}
    in 2016 (renamed to \href{https://github.blog/2016-09-07-building-resilience-in-spokes/}{Spokes})
    where they \href{https://github.blog/2016-09-07-building-resilience-in-spokes/#defining-resilience}{pay attention}
    to the consistency:
    \begin{quote}
      Spokes puts the highest priority on consistency and partition tolerance.
      In worst-case failure scenarios, it will refuse to accept writes that it cannot commit,
      synchronously, to at least two replicas.
    \end{quote}
     It is proprietary software that can't be used freely, and the source code is closed.
    Spokes papers claim that it pays attention to consistency, but on the
    \href{https://www.youtube.com/watch?v=DY0yNRNkYb0}{conference talk} they mentioned that
    it's rarely possible to break the consistency which requires manual intervention.
    Therefore the approach of distributed system design used by Spokes is not suitable for open
    source project, where the maintenance team doesn't exist.
  \item[Gitaly]
    \href{https://docs.gitlab.com/ee/README.html}{Gitlab} has
    \href{https://docs.gitlab.com/ee/administration/gitaly/}{Gitaly} service which provides
    \code{gRPC} API for Gitlab website and git-ssh proxy to perform all git operations via API.
    It's \href{https://gitlab.com/gitlab-org/gitaly}{open source} component.
    Gitaly proposed a new design for service which claims to provid
    \href{https://gitlab.com/gitlab-org/gitaly/-/blob/master/doc/design\_ha.md\#strong-consistency-design}{strong concistency}
    but in fact it doesn't provide linearizability of commands in system \todo{arguments and proves}.
    Furthermore, GitLab can change HA licensing \todo{find cases},
    or restrict HA support \href{https://news.ycombinator.com/item?id=21437334}{based on country residence}.
  \item[JGit]
    \href{https://www.eclipse.org/jgit/}{Jgit} is a Java git server created by \href{https://www.eclipse.org/}{Eclipse}.
    Google \href{https://www.eclipse.org//lists/jgit-dev/msg03073.html}{contributed} to this project with Ketch module:
    \begin{quote}
      Git Ketch is a multi-master Git repository management system. Writes
      (such as git push) can be started on any server, at any time. Writes
      are successful only if a majority of participant servers agree.
      Asked writes are durable against server failure because a majority of
      the participants are storing all required objects.
    \end{quote}
    But this is the only place where Ketch is mentioned. \todo{Analyze source code of Ketch module}.
  \item[IPFS]
    \href{https://ipfs.io/}{IPFS} is not exactly distributed git repository project but has similar ideas
    and could be helpful for us. \todo{analyze IPFS project}.
  \item[brig]
    \todo{analyze the project} \href{https://github.com/sahib/brig}{brig}.
\end{description}

\subsection{Functional Requirements}\label{sec:fr}

The most important \href{https://en.wikipedia.org/wiki/Functional_requirement}{functional requirements} are:

\begin{description}
  \item[Front end]
    The system potentically may have different kinds of front-ends,
    but it's required to support \href{https://grpc.io/}{gRPC}
    of \href{https://about.gitlab.com/}{GitLab} to integrate the system
    into GitLab service and replace
    but it's required to support file-system storage.
\end{description}

\subsection{Expected Metrics}\label{ref:metrics}

In a large enterprise it is expected to have the following
numbers, in terms of load, size, and speed:

\begin{tabular}{ll}
  Repositories & 2M \\
  Active users & 100K/day \\
  Merges & 100K/day \\
  Fetches & 15M/day, 15K/m -- peak \\
  Push & 200K/day \\
  Traffic -- download & 200Tb/day \\
  Traffic -- update & 250Gb/day \\
\end{tabular}

\include{appendix-a}

\newpage

\bibliography{references}
\bibliographystyle{ieeetr}

% REF - old references we may also include in this paper:

% Implementing Fault-Tolerant Services Using the State Machine Approach: A Tutorial, FRED B. SCHNEIDER Department of Computer Science, Cornell University, Ithaca, New York 14853 U.S.A.

% Replication Management using the State Machine Approach, Fred B. Schneider Department of Computer Science Cornell University Ithaca, NewYork 14853 U.S.A.

% Building resilience in Spokes, Patrick Reynolds, https://github.blog/2016-09-07-building-resilience-in-spokes/

% Kademlia: A Peer-to-peer information system based on the XOR Metric, Petar Maymounkov and David Mazieres New York University

% S/Kademlia: A Practicable Approach Towards Secure Key-Based RoutingIngmar Baumgart and Sebastian MiesInstitute of TelematicsUniversit at Karlsruhe (TH)\@ Dâ€“76128 Karlsruhe, Germany

% IPFS -- Content Addressed, Versioned, P2P File System (DRAFT 3) Juan Benet

\end{document}
